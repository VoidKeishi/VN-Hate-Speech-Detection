{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huấn luyện trên bộ dữ liệu ViHSD\n",
    "* Bao gồm dữ liệu thu thập từ mạng xã hội\n",
    "* Dữ liệu có tính toxic cao - phân biệt chủng tộc, vùng miền, công kích cá nhân, chửi đổng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvi\n",
    "from utility.utility import load_data\n",
    "import string\n",
    "import emoji_vietnamese  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup DATA for train and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_type = 'merged'\n",
    "train = load_data(set_name='train', dataset=dataset_train_type)\n",
    "dev = load_data(set_name='dev', dataset=dataset_train_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup DATA for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_type = 'merged'\n",
    "test = load_data(set_name='test', dataset=dataset_test_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'], dev['label'], test['label'] = train['label'].replace(2,1), dev['label'].replace(2,1), test['label'].replace(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "* Remove url in comment\n",
    "* remove punctuation\n",
    "* Lowercase data\n",
    "* Remove stopwords\n",
    "* Remove emoji\n",
    "* Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Thật tuyệt vời -> Thật tuyệt_vời\n",
    "    \"\"\"\n",
    "    return ViTokenizer.tokenize(text)\n",
    "\n",
    "# apply tokenize to text\n",
    "train['text'] = train['text'].apply(tokenize)\n",
    "dev['text'] = dev['text'].apply(tokenize)\n",
    "test['text'] = test['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    data,\n",
    "    url=True,\n",
    "    punctuation=True,\n",
    "    lowercase=True,\n",
    "    stopword=False,\n",
    "    special_stopwords=[],\n",
    "    emoji=False\n",
    "):\n",
    "    # Load stopwords\n",
    "    with open('./utility/Stopwords/vietnamese-stopwords-dash.txt', 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "    for word in special_stopwords:\n",
    "        stopwords.remove(word)\n",
    "    # Function to remove stopwords\n",
    "    def remove_stopwords(text):\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "        return ' '.join(words)\n",
    "    if url:\n",
    "        # Remove URLs\n",
    "        data['text'] = data['text'].str.replace(\n",
    "            r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "    if punctuation:\n",
    "        # Remove punctuation\n",
    "        data['text'] = data['text'].str.replace(\n",
    "            '['+string.punctuation+']', '', regex=True)\n",
    "    if lowercase:\n",
    "        # Lowercase\n",
    "        data['text'] = data['text'].str.lower()\n",
    "    if stopword:\n",
    "        # Remove stopword\n",
    "        data['text'] = data['text'].apply(remove_stopwords)\n",
    "    if emoji:\n",
    "        # Remove emojis\n",
    "        data['text'] = data['text'].apply(emoji_vietnamese.demojize)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_stopwords = [\"không\",\"không_có\",\"không_thể\",\"chưa\", \"được\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocess = preprocess_data(train,\n",
    "                                   url=True,\n",
    "                                   punctuation=True,\n",
    "                                   lowercase=True,\n",
    "                                   stopword=True,\n",
    "                                   special_stopwords=special_stopwords,\n",
    "                                   emoji=True)\n",
    "dev_preprocess = preprocess_data(dev,\n",
    "                                 url=True,\n",
    "                                 punctuation=True,\n",
    "                                 lowercase=True,\n",
    "                                 stopword=True,\n",
    "                                 special_stopwords=special_stopwords,\n",
    "                                 emoji=True)\n",
    "test_preprocess = preprocess_data(test,\n",
    "                                  url=True,\n",
    "                                  punctuation=True,\n",
    "                                  lowercase=True,\n",
    "                                  stopword=True,\n",
    "                                  special_stopwords=special_stopwords,\n",
    "                                  emoji=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['text'].astype(str)\n",
    "y_train = train['label']\n",
    "X_dev = dev['text'].astype(str)\n",
    "y_dev = dev['label']\n",
    "X_test = test['text'].astype(str)\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Để lấy dữ liệu cho nhanh chứ ko up lên git\n",
    "train_preprocess.to_csv('merge_preprocessed_train_data.csv', index=False)\n",
    "dev_preprocess.to_csv('merge_preprocessed_dev_data.csv', index=False)\n",
    "test_preprocess.to_csv('merge_preprocessed_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from csv\n",
    "* Tokenizer and pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "vocab_size=50000\n",
    "embedding_dim=128\n",
    "max_length=128\n",
    "\n",
    "train_data = pd.read_csv('merge_preprocessed_train_data.csv')\n",
    "dev_data = pd.read_csv('merge_preprocessed_dev_data.csv')\n",
    "test_data = pd.read_csv('merge_preprocessed_test_data.csv')\n",
    "\n",
    "## Lấy 1 phần dữ liệu để chạy nhanh\n",
    "# train_data = train_data[:2254]\n",
    "# dev_data = dev_data[:267]\n",
    "\n",
    "# X_train, y_train,_,_ = train_test_split(train_data['text'], train_data['label'], test_size=0.99, random_state=42)\n",
    "# X_val, y_val,_,_ = train_test_split(dev_data['text'], dev_data['label'], test_size=0.99, random_state=42)\n",
    "# X_train= X_train.astype(str)\n",
    "# X_val= X_val.astype(str)\n",
    "\n",
    "X_train, y_train = train_data['text'].astype(str), train_data['label']\n",
    "X_dev, y_dev = dev_data['text'].astype(str), dev_data['label']\n",
    "X_test, y_test = test_data['text'].astype(str), test_data['label']\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='post',truncating='post')\n",
    "\n",
    "# Thực hiện thay đổi test để đưa vào tính toán val_acc\n",
    "X_dev = tokenizer.texts_to_sequences(X_dev)\n",
    "X_dev = pad_sequences(X_dev, maxlen=max_length, padding='post',truncating='post')\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "  y_pred_rounded = torch.round(y_pred)  \n",
    "  correct = torch.eq(y_true, y_pred_rounded).sum().item()\n",
    "  acc = (correct/len(y_pred))*100\n",
    "  return acc\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "def f1_score_fn(y_true, y_pred):\n",
    "  y_true = y_true.int().tolist()\n",
    "  y_pred = torch.round(y_pred).int().tolist()\n",
    "  f1_score_pos1 = f1_score(y_true=y_true, y_pred=y_pred, pos_label=1)\n",
    "  f1_score_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "  # print(classification_report(y_true=y_true, y_pred=y_pred,zero_division=1))  \n",
    "  return f1_score_pos1, f1_score_macro_average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_encoded: np.ndarray, y_encoded: pd.core.series.Series):\n",
    "        # Setup\n",
    "        self.x_encoded = x_encoded\n",
    "        self.y_encoded = y_encoded.tolist()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.FloatTensor(self.x_encoded[idx]), self.y_encoded[idx])\n",
    "        # return (self.x_encoded[idx], self.y_encoded[idx])\n",
    "        # return {'text': self.x[idx], 'label': self.y_encoded[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x_encoded.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(X_train, y_train)\n",
    "dev_data = CustomDataset(X_dev, y_dev)\n",
    "test_data = CustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class StratifiedBatchSampler:\n",
    "    \"\"\"Stratified batch sampling\n",
    "    Provides equal representation of target classes in each batch\n",
    "    \"\"\"\n",
    "    def __init__(self, y, batch_size, shuffle=True):\n",
    "        if torch.is_tensor(y):\n",
    "            y = y.numpy()\n",
    "        assert len(y.shape) == 1, 'label array must be 1D'\n",
    "        n_batches = int(len(y) / batch_size)\n",
    "        self.skf = StratifiedKFold(n_splits=n_batches, shuffle=shuffle)\n",
    "        self.X = torch.randn(len(y),1).numpy()\n",
    "        self.y = y\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.skf.random_state = torch.randint(0,int(1e8),size=()).item()\n",
    "        for train_idx, test_idx in self.skf.split(self.X, self.y):\n",
    "            yield test_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE=32\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_sampler=StratifiedBatchSampler(torch.tensor(train_data.y_encoded), batch_size=BATCH_SIZE))\n",
    "dev_dataloader = DataLoader(dataset=dev_data,\n",
    "                              batch_sampler=StratifiedBatchSampler(torch.tensor(dev_data.y_encoded), batch_size=BATCH_SIZE))                            \n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                                batch_sampler=StratifiedBatchSampler(torch.tensor(test_data.y_encoded), batch_size=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocal_size, embedding_dim, hidden_size, output_dim, length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=vocal_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)\n",
    "        self.dropout_lstm = nn.Dropout(p=0.2)  # Dropout sau LSTM\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=length*hidden_size, out_features=length*hidden_size//4)\n",
    "        self.bn1 = nn.BatchNorm1d(length*hidden_size//4) \n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=length*hidden_size//4, out_features=length*hidden_size//16)\n",
    "        self.bn2 = nn.BatchNorm1d(length*hidden_size//16) \n",
    "\n",
    "        self.fc3 = nn.Linear(in_features=length*hidden_size//16, out_features=length*hidden_size//64)\n",
    "        self.bn3 = nn.BatchNorm1d(length*hidden_size//64) \n",
    "\n",
    "        self.fc4 = nn.Linear(in_features=length*hidden_size//64, out_features=output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout_lstm(x) # Áp dụng Dropout sau LSTM\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x) \n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embed): Embedding(50000, 128)\n",
       "  (lstm): LSTM(128, 32, batch_first=True)\n",
       "  (dropout_lstm): Dropout(p=0.2, inplace=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_Model = LSTMModel(vocal_size=vocab_size, embedding_dim=embedding_dim,hidden_size=32, length=max_length, output_dim=1)\n",
    "LSTM_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 24823, 1: 4719})\n"
     ]
    }
   ],
   "source": [
    "def weighted_binary_cross_entropy(y_true, y_pred, pos_weight):\n",
    "    \"\"\"\n",
    "    Weighted Binary Cross Entropy (WBCE) = - (w * y * log(p) + (1 - y) * log(1 - p))\n",
    "    Trong đó:\n",
    "    y: Nhãn thực tế (0 hoặc 1).\n",
    "    p: Xác suất dự đoán cho lớp positive (nhãn 1).\n",
    "    w: Trọng số cho lớp positive.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-7\n",
    "    y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)  # giới hạn giá trị dự đoán trong khoảng (epsilon, 1 - epsilon)\n",
    "    bce = - (pos_weight * y_true * torch.log(y_pred) +\n",
    "              (1 - y_true) * torch.log(1 - y_pred))     # Binary Cross Entropy trong đo tăng tầm quan trọng khi dự đoán sai lớp 1\n",
    "    return torch.mean(bce)\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(y_train))\n",
    "neg_count = Counter(y_train)[0]\n",
    "pos_count = Counter(y_train)[1]\n",
    "pos_weight = torch.tensor((neg_count/3) / pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(LSTM_Model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eefb0dbd1414597b5701530327e91cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "------\n",
      "\n",
      "Train loss: 0.0137, Train acc: 0.7500, F1 train pos 1 score: 0.5548, F1 train macro score: 0.7397 | \n",
      "Test loss: 0.0179,   Test acc: 0.7764,   F1 test pos 1 score: 0.3289, F1 test macro scoreL 0.6164\n",
      "save model at this epoch\n",
      "Epoch: 1\n",
      "------\n",
      "\n",
      "Train loss: 0.0128, Train acc: 0.7579, F1 train pos 1 score: 0.5881, F1 train macro score: 0.7610 | \n",
      "Test loss: 0.0178,   Test acc: 0.7898,   F1 test pos 1 score: 0.3158, F1 test macro scoreL 0.6136\n",
      "save model at this epoch\n",
      "Epoch: 2\n",
      "------\n",
      "\n",
      "Train loss: 0.0121, Train acc: 0.7562, F1 train pos 1 score: 0.6083, F1 train macro score: 0.7725 | \n",
      "Test loss: 0.0179,   Test acc: 0.7924,   F1 test pos 1 score: 0.3029, F1 test macro scoreL 0.6072\n",
      "Epoch: 3\n",
      "------\n",
      "\n",
      "Train loss: 0.0112, Train acc: 0.7540, F1 train pos 1 score: 0.6512, F1 train macro score: 0.7970 | \n",
      "Test loss: 0.0180,   Test acc: 0.7945,   F1 test pos 1 score: 0.3139, F1 test macro scoreL 0.6139\n",
      "save model at this epoch\n",
      "Epoch: 4\n",
      "------\n",
      "\n",
      "Train loss: 0.0106, Train acc: 0.7512, F1 train pos 1 score: 0.6755, F1 train macro score: 0.8107 | \n",
      "Test loss: 0.0181,   Test acc: 0.7922,   F1 test pos 1 score: 0.3117, F1 test macro scoreL 0.6125\n",
      "Epoch: 5\n",
      "------\n",
      "\n",
      "Train loss: 0.0099, Train acc: 0.7509, F1 train pos 1 score: 0.6992, F1 train macro score: 0.8245 | \n",
      "Test loss: 0.0189,   Test acc: 0.7974,   F1 test pos 1 score: 0.3054, F1 test macro scoreL 0.6103\n",
      "Epoch: 6\n",
      "------\n",
      "\n",
      "Train loss: 0.0094, Train acc: 0.7480, F1 train pos 1 score: 0.7148, F1 train macro score: 0.8332 | \n",
      "Test loss: 0.0188,   Test acc: 0.7938,   F1 test pos 1 score: 0.3269, F1 test macro scoreL 0.6210\n",
      "save model at this epoch\n",
      "Epoch: 7\n",
      "------\n",
      "\n",
      "Train loss: 0.0091, Train acc: 0.7454, F1 train pos 1 score: 0.7247, F1 train macro score: 0.8386 | \n",
      "Test loss: 0.0193,   Test acc: 0.7959,   F1 test pos 1 score: 0.3360, F1 test macro scoreL 0.6268\n",
      "save model at this epoch\n",
      "Epoch: 8\n",
      "------\n",
      "\n",
      "Train loss: 0.0084, Train acc: 0.7449, F1 train pos 1 score: 0.7482, F1 train macro score: 0.8524 | \n",
      "Test loss: 0.0195,   Test acc: 0.7946,   F1 test pos 1 score: 0.3200, F1 test macro scoreL 0.6177\n",
      "Epoch: 9\n",
      "------\n",
      "\n",
      "Train loss: 0.0080, Train acc: 0.7418, F1 train pos 1 score: 0.7667, F1 train macro score: 0.8627 | \n",
      "Test loss: 0.0196,   Test acc: 0.7794,   F1 test pos 1 score: 0.3465, F1 test macro scoreL 0.6279\n",
      "Epoch: 10\n",
      "------\n",
      "\n",
      "Train loss: 0.0076, Train acc: 0.7394, F1 train pos 1 score: 0.7768, F1 train macro score: 0.8684 | \n",
      "Test loss: 0.0201,   Test acc: 0.7849,   F1 test pos 1 score: 0.3519, F1 test macro scoreL 0.6325\n",
      "save model at this epoch\n",
      "Epoch: 11\n",
      "------\n",
      "\n",
      "Train loss: 0.0075, Train acc: 0.7394, F1 train pos 1 score: 0.7845, F1 train macro score: 0.8730 | \n",
      "Test loss: 0.0204,   Test acc: 0.7842,   F1 test pos 1 score: 0.3613, F1 test macro scoreL 0.6370\n",
      "save model at this epoch\n",
      "Epoch: 12\n",
      "------\n",
      "\n",
      "Train loss: 0.0100, Train acc: 0.7419, F1 train pos 1 score: 0.6969, F1 train macro score: 0.8215 | \n",
      "Test loss: 0.0209,   Test acc: 0.7751,   F1 test pos 1 score: 0.3650, F1 test macro scoreL 0.6368\n",
      "Epoch: 13\n",
      "------\n",
      "\n",
      "Train loss: 0.0097, Train acc: 0.7400, F1 train pos 1 score: 0.7037, F1 train macro score: 0.8253 | \n",
      "Test loss: 0.0211,   Test acc: 0.7829,   F1 test pos 1 score: 0.3668, F1 test macro scoreL 0.6404\n",
      "save model at this epoch\n",
      "Epoch: 14\n",
      "------\n",
      "\n",
      "Train loss: 0.0092, Train acc: 0.7397, F1 train pos 1 score: 0.7246, F1 train macro score: 0.8376 | \n",
      "Test loss: 0.0211,   Test acc: 0.7941,   F1 test pos 1 score: 0.3315, F1 test macro scoreL 0.6245\n"
     ]
    }
   ],
   "source": [
    "# Write a training and evaluationg loop for model_1\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Train for longer\n",
    "epochs = 15\n",
    "\n",
    "# # Put data on the target device\n",
    "# X_padded_sequences, y_train = torch.tensor(X_padded_sequences).to(device), torch.tensor(y_train).to(device)\n",
    "# padded_val_sequences, y_test=  torch.tensor(padded_val_sequences).to(device), torch.tensor(y_test).to(device)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "best_f1_score = 0\n",
    "best_acc_score = 0\n",
    "\n",
    "# Create training and test loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  print(f\"Epoch: {epoch}\\n------\")\n",
    "  ### Training\n",
    "  train_loss, train_acc=0,0\n",
    "  f1_train_pos_1 = []\n",
    "  f1_train_macro = []\n",
    "  cnt = 0\n",
    "  f1_score_list = []\n",
    "  LSTM_Model.train()\n",
    "  # Add a loop to loop through the training batches\n",
    "  for batch, (X, y) in enumerate(train_dataloader):\n",
    "    cnt+=1\n",
    "    # 1. Forward\n",
    "    X = X.long()\n",
    "    y_pred = LSTM_Model(X)\n",
    "\n",
    "    # 2. Calculate the loss\n",
    "    # loss = loss_fn(y_pred.squeeze(), y.float().squeeze())\n",
    "    loss = weighted_binary_cross_entropy(y_true=y.float().squeeze(), y_pred=y_pred.squeeze(), pos_weight=pos_weight)\n",
    "    train_loss += loss.item()\n",
    "    f1_pos_1, f1_macro = f1_score_fn(y_true= y.float(),\n",
    "                            y_pred = y_pred.squeeze(dim=1))\n",
    "    f1_train_pos_1.append(f1_pos_1.item())\n",
    "    f1_train_macro.append(f1_macro.item())\n",
    "\n",
    "    # 3.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4.\n",
    "    loss.backward()\n",
    "\n",
    "    # 5.\n",
    "    optimizer.step()\n",
    "\n",
    "    # 6. Calculate accuracy metric\n",
    "    y_pred_class = torch.round(y_pred)\n",
    "    train_acc += (y_pred_class==y).sum().item()/len(y_pred_class)\n",
    "\n",
    "  # Divide total train loss by length of train dataloader\n",
    "  # train_loss\n",
    "  train_loss /= len(train_dataloader)\n",
    "  train_acc /= len(train_dataloader)\n",
    "  f1_train_pos_1 = sum(f1_train_pos_1)/len(f1_train_pos_1)\n",
    "  f1_train_macro = sum(f1_train_macro)/len(f1_train_macro)\n",
    "\n",
    "  ### Testing\n",
    "  test_loss, test_acc = 0,0\n",
    "  f1_test_pos_1 = []\n",
    "  f1_test_macro = []\n",
    "  LSTM_Model.eval()\n",
    "  with torch.inference_mode():\n",
    "    for batch, (X_test_, y_test_) in enumerate(dev_dataloader):\n",
    "      # 1. Forward pass\n",
    "      X_test_ = X_test_.long()\n",
    "      test_pred = LSTM_Model(X_test_)\n",
    "\n",
    "      # 2. Calculate the loss (accumulatively)\n",
    "      test_loss += weighted_binary_cross_entropy(\n",
    "                y_test_.float(), test_pred.squeeze(dim=1), pos_weight)\n",
    "      # print(test_pred.shape)\n",
    "      # 3. Calculate accuracy\n",
    "      acc = accuracy_fn(y_true= y_test_.float(),\n",
    "                        y_pred = test_pred.squeeze(dim=1))\n",
    "      acc = (torch.round(test_pred)==y_test_).sum().item()/len(y_test_)\n",
    "      test_acc += acc\n",
    "      # 4. Calculate f1 score\n",
    "      pos_1, macro = f1_score_fn(y_true= y_test_.float(),\n",
    "                              y_pred = test_pred.squeeze(dim=1))\n",
    "      f1_test_pos_1.append(pos_1.item())\n",
    "      f1_test_macro.append(macro.item())\n",
    "\n",
    "    # Calculate the test loss average per batch\n",
    "    test_loss /= len(dev_dataloader)\n",
    "    # test_loss /= len(test_dataloader)\n",
    "    f1_test_pos_1 = sum(f1_test_pos_1)/len(f1_test_pos_1)\n",
    "    f1_test_macro = sum(f1_test_macro)/len(f1_test_macro)\n",
    "    # Calculate the test acc average per batch\n",
    "    test_acc /= len(dev_dataloader)\n",
    "\n",
    "  # print out what happen\n",
    "  print(f\"\\nTrain loss: {train_loss:.4f}, Train acc: {train_acc:.4f}, F1 train pos 1 score: {f1_train_pos_1:.4f}, F1 train macro score: {f1_train_macro:.4f} | \\nTest loss: {test_loss:.4f},   Test acc: {test_acc:.4f},   F1 test pos 1 score: {f1_test_pos_1:.4f}, F1 test macro scoreL {f1_test_macro:.4f}\")\n",
    "  # print(f\"\\nTrain loss; {train_loss:.4f}\")\n",
    "\n",
    "  # Lưu trữ trọng số mô hình tốt nhất\n",
    "  if f1_test_pos_1 > best_f1_score-0.03 and test_acc > best_acc_score-0.03 and f1_test_pos_1 + test_acc > best_f1_score+best_acc_score:\n",
    "    best_f1_score = f1_test_pos_1\n",
    "    best_acc_score = test_acc\n",
    "    torch.save(LSTM_Model.state_dict(), f\"best_model_lstm_merge_acc{best_acc_score:.2f}_mf1{best_f1_score:.2f}.pth\")\n",
    "    print(\"save model at this epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lưu model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embed.weight',\n",
       "              tensor([[ 0.4620,  0.3414, -1.1506,  ..., -0.0948,  0.0100,  0.0841],\n",
       "                      [-0.9077, -0.0030,  0.3332,  ..., -1.3447, -0.9163,  0.4607],\n",
       "                      [ 1.3016, -1.0711, -2.2096,  ..., -1.4831,  0.8135,  1.3733],\n",
       "                      ...,\n",
       "                      [ 1.2398, -0.2819, -0.9345,  ...,  0.9117, -0.1086, -0.0961],\n",
       "                      [ 1.4673,  0.1206,  2.0751,  ...,  0.5387, -0.3628, -0.2128],\n",
       "                      [ 1.1740, -0.2834, -1.2699,  ..., -0.0842, -0.4668, -0.9259]])),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[-0.0764,  0.0522,  0.1317,  ...,  0.0781,  0.1752, -0.1137],\n",
       "                      [-0.0812,  0.0698, -0.0491,  ..., -0.1185,  0.0259,  0.0752],\n",
       "                      [ 0.0559, -0.0294,  0.0390,  ...,  0.0301, -0.1181,  0.1195],\n",
       "                      ...,\n",
       "                      [-0.0900, -0.0937,  0.0126,  ...,  0.1573, -0.1222,  0.0071],\n",
       "                      [-0.0484,  0.0378,  0.0243,  ...,  0.0079,  0.0066,  0.1540],\n",
       "                      [-0.0972,  0.0948,  0.1884,  ...,  0.0289,  0.1629,  0.1113]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[-0.1488,  0.0894, -0.0955,  ...,  0.1669, -0.1618,  0.1064],\n",
       "                      [-0.1317, -0.0692, -0.1161,  ..., -0.0622, -0.1613,  0.0834],\n",
       "                      [ 0.0499,  0.1554, -0.0901,  ..., -0.0330,  0.0385, -0.0288],\n",
       "                      ...,\n",
       "                      [-0.0510,  0.0352,  0.0171,  ...,  0.1310,  0.0253,  0.1753],\n",
       "                      [-0.0434, -0.0090,  0.0838,  ..., -0.0862, -0.1712,  0.0844],\n",
       "                      [-0.0421, -0.1058,  0.0029,  ..., -0.1260, -0.1271,  0.0592]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([-0.0553, -0.0363, -0.0076, -0.1749, -0.1386,  0.0795,  0.1463, -0.1459,\n",
       "                       0.1282, -0.0255,  0.0685, -0.1882,  0.1525, -0.1836,  0.1641, -0.0069,\n",
       "                      -0.1395,  0.1535, -0.0580,  0.0584, -0.1521, -0.1515, -0.0024, -0.1521,\n",
       "                      -0.1890,  0.0224, -0.1583,  0.0556, -0.0757, -0.0504,  0.1228,  0.0247,\n",
       "                      -0.1868, -0.1362,  0.0958, -0.1027,  0.1417, -0.1764,  0.1015, -0.0957,\n",
       "                       0.1074,  0.0673, -0.1593,  0.1682, -0.1328,  0.1406, -0.0601, -0.0880,\n",
       "                      -0.0193, -0.1834,  0.1106,  0.0045, -0.1219,  0.0298,  0.0764,  0.0724,\n",
       "                       0.1401, -0.1800,  0.1196,  0.0426, -0.1533, -0.1866, -0.1454, -0.1437,\n",
       "                       0.0883, -0.1350, -0.0260,  0.1463, -0.1515, -0.1788, -0.0840, -0.0510,\n",
       "                       0.1623,  0.0193,  0.1501, -0.1524, -0.0123,  0.0622, -0.0479, -0.1518,\n",
       "                      -0.1110, -0.0084,  0.0294, -0.1458,  0.0381,  0.1042,  0.1295,  0.0888,\n",
       "                      -0.1295, -0.1148,  0.0289, -0.1591, -0.1501, -0.0271, -0.1647,  0.1218,\n",
       "                      -0.1488, -0.1133, -0.1404,  0.0290, -0.1820, -0.1141, -0.1444,  0.1192,\n",
       "                      -0.1338, -0.0722, -0.0113,  0.0631, -0.1737, -0.1784, -0.0308, -0.0214,\n",
       "                      -0.1174, -0.0344,  0.0620,  0.0933,  0.0315,  0.0617,  0.1598, -0.0695,\n",
       "                       0.0891,  0.1042,  0.1623,  0.1101, -0.0843,  0.0715,  0.0089, -0.1775])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 0.0014,  0.0420, -0.0264,  0.0263,  0.0778, -0.0500,  0.0455, -0.0940,\n",
       "                      -0.0542,  0.1333, -0.0893, -0.1776,  0.0052, -0.1502, -0.1548, -0.0532,\n",
       "                      -0.1023,  0.0771,  0.0841,  0.0302, -0.1279,  0.1042,  0.0883, -0.1000,\n",
       "                      -0.0339,  0.1038,  0.0223, -0.1699, -0.1318, -0.1701,  0.0335, -0.0034,\n",
       "                      -0.1348, -0.1471, -0.1283,  0.0301, -0.1085, -0.0092,  0.1640, -0.1859,\n",
       "                       0.0892, -0.1153, -0.0821, -0.0149, -0.0515, -0.0157,  0.0416, -0.1388,\n",
       "                       0.1388,  0.0024,  0.1152, -0.0889,  0.0913, -0.0971,  0.1200, -0.0126,\n",
       "                       0.1529, -0.1510, -0.0672,  0.1366, -0.0988, -0.0200, -0.1021, -0.1868,\n",
       "                       0.1328, -0.1009,  0.1324, -0.0952,  0.0019,  0.0719,  0.0975, -0.1427,\n",
       "                       0.0841,  0.0581,  0.1693,  0.1585, -0.0449, -0.0533, -0.0281,  0.1047,\n",
       "                      -0.1434,  0.1281, -0.1579, -0.1424, -0.0472, -0.0217, -0.1195,  0.0361,\n",
       "                       0.1547, -0.1354,  0.0236, -0.0367,  0.0962,  0.0644, -0.1462, -0.1536,\n",
       "                       0.1086, -0.1152,  0.0904,  0.0221, -0.1007,  0.0755,  0.0011, -0.0890,\n",
       "                       0.0670, -0.0409, -0.1454, -0.1803,  0.0821, -0.0973, -0.0984,  0.0271,\n",
       "                      -0.0956, -0.0513, -0.0245, -0.1724, -0.0279,  0.1147, -0.0023, -0.0913,\n",
       "                       0.0980, -0.0168, -0.1261,  0.0440,  0.0672,  0.0376, -0.0536, -0.1537])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[-0.0015, -0.0092, -0.0019,  ...,  0.0022,  0.0041, -0.0065],\n",
       "                      [ 0.0080, -0.0024,  0.0059,  ..., -0.0031,  0.0133, -0.0167],\n",
       "                      [ 0.0062,  0.0168,  0.0038,  ...,  0.0163, -0.0139, -0.0105],\n",
       "                      ...,\n",
       "                      [-0.0034,  0.0022,  0.0194,  ...,  0.0108, -0.0070, -0.0053],\n",
       "                      [ 0.0148,  0.0046,  0.0085,  ..., -0.0050,  0.0141,  0.0006],\n",
       "                      [ 0.0094,  0.0139, -0.0094,  ..., -0.0043,  0.0080,  0.0079]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.0053, -0.0100, -0.0066,  ..., -0.0144,  0.0083, -0.0075])),\n",
       "             ('bn1.weight',\n",
       "              tensor([1.0031, 0.9988, 1.0010,  ..., 1.0023, 0.9979, 0.9985])),\n",
       "             ('bn1.bias',\n",
       "              tensor([ 0.0014, -0.0051, -0.0048,  ..., -0.0005, -0.0059, -0.0017])),\n",
       "             ('bn1.running_mean',\n",
       "              tensor([ 0.0103, -0.0325, -0.0160,  ..., -0.0171, -0.0002, -0.0187])),\n",
       "             ('bn1.running_var',\n",
       "              tensor([0.0014, 0.0012, 0.0016,  ..., 0.0017, 0.0016, 0.0013])),\n",
       "             ('bn1.num_batches_tracked', tensor(25006)),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.0134,  0.0141,  0.0270,  ...,  0.0169, -0.0278, -0.0276],\n",
       "                      [-0.0249, -0.0263,  0.0070,  ...,  0.0033, -0.0018, -0.0024],\n",
       "                      [-0.0205, -0.0015, -0.0049,  ..., -0.0063, -0.0277, -0.0182],\n",
       "                      ...,\n",
       "                      [-0.0050, -0.0191, -0.0227,  ..., -0.0061, -0.0346, -0.0266],\n",
       "                      [ 0.0121, -0.0084, -0.0003,  ..., -0.0288,  0.0138, -0.0071],\n",
       "                      [ 0.0219,  0.0187,  0.0037,  ...,  0.0167,  0.0070, -0.0158]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.0120,  0.0073,  0.0177, -0.0276, -0.0120, -0.0016,  0.0189,  0.0143,\n",
       "                      -0.0139,  0.0011,  0.0120, -0.0003, -0.0087,  0.0251, -0.0287, -0.0100,\n",
       "                       0.0068, -0.0256, -0.0189, -0.0239, -0.0060,  0.0039,  0.0281,  0.0165,\n",
       "                       0.0174, -0.0229,  0.0255, -0.0196,  0.0228, -0.0291, -0.0156,  0.0080,\n",
       "                      -0.0210,  0.0248, -0.0210,  0.0262, -0.0016, -0.0180,  0.0170, -0.0123,\n",
       "                      -0.0303,  0.0067,  0.0252, -0.0146, -0.0097,  0.0272,  0.0301, -0.0277,\n",
       "                       0.0206, -0.0241,  0.0174, -0.0088,  0.0150, -0.0235, -0.0154, -0.0076,\n",
       "                       0.0071,  0.0227,  0.0127, -0.0028, -0.0207, -0.0055,  0.0007, -0.0265,\n",
       "                      -0.0280, -0.0228, -0.0129,  0.0300, -0.0027, -0.0046,  0.0065, -0.0305,\n",
       "                       0.0021,  0.0242,  0.0255, -0.0035,  0.0050, -0.0277, -0.0230,  0.0213,\n",
       "                       0.0205,  0.0247, -0.0178,  0.0270,  0.0143, -0.0244,  0.0038,  0.0063,\n",
       "                       0.0019, -0.0145,  0.0293, -0.0239, -0.0133, -0.0222, -0.0148, -0.0302,\n",
       "                       0.0217, -0.0289, -0.0076,  0.0128, -0.0205, -0.0310,  0.0009, -0.0204,\n",
       "                       0.0202,  0.0110, -0.0282,  0.0033,  0.0023,  0.0074, -0.0273, -0.0196,\n",
       "                       0.0138,  0.0208,  0.0107,  0.0262, -0.0251,  0.0230,  0.0202, -0.0308,\n",
       "                      -0.0312,  0.0281, -0.0101, -0.0052, -0.0275,  0.0213, -0.0021, -0.0312,\n",
       "                       0.0237,  0.0208,  0.0011,  0.0226,  0.0163,  0.0150, -0.0164,  0.0211,\n",
       "                      -0.0309,  0.0162,  0.0306,  0.0024,  0.0070,  0.0214, -0.0241,  0.0211,\n",
       "                      -0.0191,  0.0264, -0.0232, -0.0008,  0.0272,  0.0282, -0.0126, -0.0206,\n",
       "                       0.0285,  0.0060, -0.0116, -0.0039,  0.0035,  0.0294, -0.0284, -0.0197,\n",
       "                       0.0016,  0.0206,  0.0222, -0.0172,  0.0299, -0.0112, -0.0254,  0.0235,\n",
       "                      -0.0073,  0.0201,  0.0306,  0.0044,  0.0142, -0.0107, -0.0165,  0.0114,\n",
       "                       0.0274, -0.0063, -0.0168, -0.0043, -0.0290, -0.0246, -0.0063, -0.0177,\n",
       "                      -0.0109, -0.0041, -0.0102, -0.0276, -0.0193,  0.0164,  0.0264,  0.0182,\n",
       "                       0.0242, -0.0273,  0.0261,  0.0175,  0.0182,  0.0114,  0.0234, -0.0280,\n",
       "                      -0.0043,  0.0028,  0.0139, -0.0175,  0.0090, -0.0282,  0.0114, -0.0098,\n",
       "                      -0.0221,  0.0218,  0.0047,  0.0262, -0.0205, -0.0294, -0.0240, -0.0139,\n",
       "                       0.0070,  0.0168, -0.0307,  0.0202, -0.0094,  0.0217, -0.0288, -0.0178,\n",
       "                       0.0227,  0.0009,  0.0307, -0.0095,  0.0149,  0.0109, -0.0257, -0.0210,\n",
       "                      -0.0011,  0.0126, -0.0031,  0.0273, -0.0190, -0.0204, -0.0143,  0.0038,\n",
       "                      -0.0153,  0.0020,  0.0211, -0.0033, -0.0015, -0.0089, -0.0108,  0.0267,\n",
       "                       0.0111,  0.0309,  0.0275, -0.0137,  0.0076,  0.0090,  0.0257,  0.0103])),\n",
       "             ('bn2.weight',\n",
       "              tensor([1.0048, 1.0047, 1.0080, 1.0000, 0.9992, 1.0119, 1.0027, 1.0048, 0.9951,\n",
       "                      1.0075, 0.9968, 1.0047, 0.9996, 0.9996, 0.9991, 0.9999, 0.9921, 1.0084,\n",
       "                      1.0036, 0.9904, 0.9977, 0.9829, 1.0016, 0.9949, 0.9860, 0.9962, 1.0057,\n",
       "                      1.0039, 1.0026, 0.9896, 1.0041, 1.0081, 1.0079, 0.9869, 0.9939, 1.0012,\n",
       "                      0.9945, 1.0005, 0.9964, 1.0081, 1.0036, 1.0033, 1.0008, 0.9935, 0.9989,\n",
       "                      1.0040, 1.0065, 1.0017, 1.0004, 0.9997, 0.9971, 1.0049, 1.0104, 0.9953,\n",
       "                      0.9934, 0.9957, 0.9906, 0.9936, 1.0009, 0.9964, 0.9965, 0.9854, 0.9988,\n",
       "                      1.0021, 0.9938, 0.9917, 1.0127, 1.0047, 0.9788, 1.0015, 0.9957, 1.0003,\n",
       "                      1.0085, 1.0089, 0.9879, 0.9936, 1.0125, 1.0000, 1.0079, 1.0002, 1.0026,\n",
       "                      1.0050, 0.9930, 1.0013, 1.0116, 0.9984, 0.9891, 0.9907, 0.9897, 0.9990,\n",
       "                      1.0018, 0.9888, 1.0018, 1.0041, 0.9915, 1.0030, 0.9988, 0.9897, 1.0068,\n",
       "                      0.9925, 0.9967, 1.0141, 0.9885, 0.9908, 1.0063, 0.9884, 1.0013, 0.9909,\n",
       "                      0.9968, 1.0090, 0.9851, 1.0030, 0.9973, 0.9957, 0.9977, 0.9996, 1.0021,\n",
       "                      1.0116, 0.9864, 1.0001, 0.9897, 0.9961, 0.9987, 1.0038, 1.0103, 0.9925,\n",
       "                      0.9890, 0.9924, 1.0120, 1.0066, 0.9939, 0.9973, 1.0095, 0.9982, 1.0008,\n",
       "                      1.0067, 0.9967, 1.0032, 0.9917, 0.9855, 0.9811, 0.9918, 0.9878, 0.9938,\n",
       "                      1.0076, 0.9998, 0.9980, 1.0078, 1.0018, 0.9978, 0.9869, 0.9918, 1.0017,\n",
       "                      0.9990, 1.0082, 1.0010, 0.9937, 0.9928, 1.0099, 0.9992, 0.9888, 1.0033,\n",
       "                      1.0077, 0.9903, 0.9955, 1.0011, 1.0012, 1.0091, 0.9973, 0.9925, 0.9997,\n",
       "                      0.9995, 0.9873, 0.9971, 0.9852, 1.0066, 0.9894, 1.0056, 0.9859, 1.0041,\n",
       "                      0.9907, 1.0112, 0.9903, 1.0044, 0.9942, 0.9996, 1.0018, 1.0108, 1.0039,\n",
       "                      0.9987, 1.0045, 0.9911, 0.9856, 1.0126, 0.9992, 1.0059, 0.9887, 0.9953,\n",
       "                      0.9932, 0.9958, 0.9999, 1.0069, 0.9996, 1.0118, 1.0004, 0.9988, 1.0003,\n",
       "                      0.9953, 0.9975, 0.9995, 1.0011, 1.0033, 0.9988, 0.9945, 0.9948, 1.0061,\n",
       "                      0.9978, 1.0050, 1.0057, 0.9880, 1.0012, 1.0005, 0.9963, 0.9882, 0.9987,\n",
       "                      0.9988, 1.0004, 0.9937, 0.9962, 0.9982, 1.0051, 0.9855, 1.0050, 0.9894,\n",
       "                      1.0067, 0.9934, 1.0026, 0.9898, 1.0048, 0.9924, 1.0044, 0.9947, 0.9987,\n",
       "                      0.9996, 0.9982, 0.9971, 0.9967, 1.0033, 1.0121, 0.9886, 0.9859, 0.9942,\n",
       "                      1.0071, 0.9928, 0.9878, 0.9829])),\n",
       "             ('bn2.bias',\n",
       "              tensor([-1.7665e-02, -2.9200e-03, -1.7153e-02,  7.0395e-05, -1.5545e-02,\n",
       "                      -1.4529e-02, -5.9158e-04,  3.2086e-03, -5.7724e-03, -3.2857e-03,\n",
       "                      -1.4803e-02, -6.7856e-03, -9.1236e-04, -1.0246e-02, -5.2329e-03,\n",
       "                      -9.5518e-03, -1.1357e-02, -1.8659e-02, -8.7703e-03, -7.3882e-03,\n",
       "                      -1.1635e-02, -1.4654e-02, -1.8297e-02, -7.1608e-03, -6.7085e-03,\n",
       "                      -4.8728e-03, -1.1069e-02, -2.8947e-03, -1.5295e-02, -2.2489e-02,\n",
       "                      -3.6530e-04, -1.9345e-02, -1.5743e-02, -1.4734e-02, -4.8898e-03,\n",
       "                      -7.4867e-03, -4.0655e-03, -3.1026e-03, -9.4439e-03, -6.7126e-03,\n",
       "                      -4.4458e-03,  4.5979e-03, -2.3235e-03, -1.4840e-02, -4.8938e-03,\n",
       "                      -1.2479e-03, -7.0946e-03, -2.0227e-02,  8.3346e-04, -1.1694e-03,\n",
       "                      -1.1995e-02, -3.4568e-03, -5.8321e-03, -7.8669e-03, -8.8050e-03,\n",
       "                      -1.1510e-02, -1.4550e-02, -8.4392e-03, -1.5740e-02, -5.6030e-03,\n",
       "                      -6.5635e-03, -1.2738e-02, -1.5270e-02, -1.4127e-02, -5.4002e-03,\n",
       "                      -2.3875e-02,  4.0950e-04,  8.2175e-04, -1.1349e-02, -1.5589e-02,\n",
       "                      -3.5764e-03, -1.1104e-02, -3.0732e-04, -2.1252e-03, -9.6262e-03,\n",
       "                      -1.0940e-02, -1.2980e-03, -6.9878e-03, -2.0629e-02, -5.1025e-03,\n",
       "                      -1.9184e-02, -1.1489e-02, -8.5299e-03, -7.1636e-03, -1.6609e-02,\n",
       "                      -1.7545e-02, -8.8082e-03, -9.4032e-03, -1.4910e-02, -8.3973e-03,\n",
       "                      -1.7029e-02, -8.8478e-03,  2.1291e-03, -1.7124e-02, -7.1175e-03,\n",
       "                      -9.6833e-03, -1.2169e-02, -1.0955e-02, -1.2020e-02, -5.0410e-03,\n",
       "                      -1.5754e-02, -2.9932e-03, -1.3208e-02, -1.2903e-02, -1.1165e-02,\n",
       "                      -8.7294e-03, -3.2265e-03, -1.4278e-02, -1.2700e-02, -4.0535e-03,\n",
       "                      -1.0483e-02, -1.3225e-02, -8.4483e-03, -5.2998e-03, -9.7085e-03,\n",
       "                      -1.1642e-02,  2.6227e-03, -9.0527e-03, -9.7470e-03, -7.6801e-03,\n",
       "                      -6.2269e-03, -8.0426e-03, -3.6080e-03, -1.0741e-02, -8.9111e-03,\n",
       "                      -1.0795e-02, -5.7166e-03, -6.2851e-03, -1.7516e-02, -8.3507e-03,\n",
       "                      -3.8389e-03, -1.0264e-02, -2.2142e-02, -5.7342e-03, -6.4740e-03,\n",
       "                      -2.7359e-03, -1.0629e-02, -4.8407e-03, -8.0829e-03, -8.1699e-03,\n",
       "                      -8.5990e-03, -9.6701e-03, -7.9837e-03, -1.2849e-02, -1.5512e-03,\n",
       "                      -9.5835e-03, -4.1533e-03, -2.9559e-03, -5.0620e-03, -1.5058e-02,\n",
       "                      -1.2491e-02, -9.4192e-03, -3.7040e-03, -4.1505e-03, -1.3382e-03,\n",
       "                      -6.2039e-03, -1.1403e-02, -3.3484e-03, -3.1301e-03, -1.6751e-02,\n",
       "                      -1.1739e-02, -2.4958e-03, -7.1601e-03, -8.9231e-03, -1.1289e-02,\n",
       "                      -1.6741e-02, -3.9704e-03, -1.5756e-02, -1.3006e-02, -1.5580e-02,\n",
       "                      -4.6993e-03, -6.3941e-03, -1.8366e-02, -7.0973e-03, -1.1096e-02,\n",
       "                      -4.3606e-03, -1.2948e-02, -1.8140e-03, -1.5619e-02, -5.3848e-03,\n",
       "                      -9.9984e-03, -2.1868e-03, -8.4043e-03, -2.6939e-03, -1.2422e-02,\n",
       "                      -8.2139e-03, -8.3164e-03,  3.4245e-03, -1.4132e-02, -6.5033e-03,\n",
       "                      -3.9461e-03, -4.4230e-03, -1.5670e-02,  8.8177e-04, -6.0967e-03,\n",
       "                      -8.1884e-03, -1.6340e-02, -5.3121e-03, -1.0211e-02, -6.5698e-03,\n",
       "                      -2.2775e-03, -3.8347e-04, -2.4848e-03, -2.3281e-02, -1.1053e-02,\n",
       "                      -1.5375e-02, -1.9756e-03, -7.9482e-03, -1.4514e-02, -9.9312e-03,\n",
       "                      -1.3061e-02, -1.4943e-03, -1.8745e-02, -8.0177e-03, -1.3301e-02,\n",
       "                      -4.7182e-03, -3.5895e-03, -1.5085e-03,  2.4792e-03, -9.2517e-03,\n",
       "                      -1.3374e-02, -4.3703e-03, -1.1318e-02, -1.9251e-02,  3.1684e-04,\n",
       "                      -2.0506e-02,  4.1529e-03, -1.2220e-02, -8.7595e-03, -9.5095e-03,\n",
       "                      -4.7910e-03, -1.0500e-02, -4.6548e-03, -1.1544e-02, -6.0538e-04,\n",
       "                      -1.3647e-02,  5.3996e-04, -3.7643e-03, -2.0596e-02, -7.5988e-03,\n",
       "                      -5.8721e-03, -8.2643e-03, -4.5535e-03, -4.4321e-03, -2.0372e-02,\n",
       "                      -4.3038e-03, -8.2000e-03, -1.8438e-02,  1.8208e-03, -1.3708e-02,\n",
       "                      -1.1421e-02, -2.2527e-02, -5.6123e-04, -7.6959e-03, -1.0908e-02,\n",
       "                      -9.8783e-03])),\n",
       "             ('bn2.running_mean',\n",
       "              tensor([ 0.3107, -0.2231, -0.2767, -0.0213,  0.0298,  0.2890, -0.2612, -0.2911,\n",
       "                      -0.3617, -0.3088,  0.1434, -0.2086, -0.1427, -0.1808, -0.3856, -0.1630,\n",
       "                      -0.5048,  0.1854,  0.1650, -0.2883,  0.1594,  0.5101, -0.3866, -0.4204,\n",
       "                       0.1010, -0.4127,  0.5457, -0.0236, -0.0570,  0.1183, -0.3318, -0.3186,\n",
       "                       0.3356,  0.0407, -0.0077,  0.5489, -0.1322, -0.1425, -0.1196,  0.2663,\n",
       "                      -0.4282, -0.0812, -0.4086, -0.1476, -0.4078, -0.2454, -0.0344,  0.1100,\n",
       "                      -0.0913, -0.0297, -0.0757, -0.3025, -0.1004, -0.0592, -0.2995,  0.2403,\n",
       "                       0.2370, -0.4538,  0.2354, -0.2301, -0.2688,  0.0013,  0.1848,  0.3496,\n",
       "                      -0.4785, -0.0512, -0.4013, -0.0622,  0.0905,  0.1811,  0.0246,  0.2639,\n",
       "                      -0.1380,  0.2296,  0.3666, -0.0127,  0.0171,  0.1431,  0.1877, -0.4137,\n",
       "                       0.3408,  0.1606,  0.3571, -0.4777,  0.1506, -0.3453,  0.2715, -0.3213,\n",
       "                      -0.3879, -0.2134,  0.1995,  0.4404, -0.0514, -0.1395, -0.7710, -0.0841,\n",
       "                       0.1821,  0.1574, -0.0537, -0.3830, -0.4252,  0.1888, -0.1274, -0.0666,\n",
       "                      -0.2300,  0.2373, -0.1288, -0.0905, -0.1518,  0.0302, -0.1728,  0.2644,\n",
       "                      -0.0552, -0.3445, -0.1770, -0.0135, -0.2230, -0.1925, -0.3031,  0.0827,\n",
       "                      -0.2063,  0.0428, -0.4764, -0.2126, -0.0710,  0.1365, -0.0625, -0.3128,\n",
       "                       0.0604,  0.2136, -0.1006, -0.2080, -0.2770, -0.1838,  0.0119, -0.0763,\n",
       "                      -0.1340, -0.2129,  0.2773,  0.2187,  0.0242, -0.1572, -0.4135, -0.2025,\n",
       "                       0.0487,  0.0757, -0.1431, -0.0044, -0.0525,  0.0133, -0.3894,  0.0092,\n",
       "                      -0.3995, -0.4697,  0.4329,  0.1295, -0.1343,  0.5016, -0.3365, -0.0384,\n",
       "                      -0.4044,  0.2456,  0.1136, -0.3194,  0.2287,  0.0092, -0.3643,  0.0894,\n",
       "                      -0.0159, -0.3702, -0.8410, -0.4657,  0.3449,  0.3103,  0.2471, -0.0537,\n",
       "                      -0.0647, -0.0577, -0.3092, -0.1352, -0.0549, -0.2728, -0.5785,  0.0852,\n",
       "                       0.0917, -0.0610, -0.1654, -0.1439, -0.0127, -0.3659,  0.3293, -0.3899,\n",
       "                       0.3026, -0.1373,  0.2141, -0.1833, -0.1272,  0.3928,  0.3433, -0.2167,\n",
       "                      -0.2552, -0.2445,  0.0341, -0.1201,  0.4010, -0.3373, -0.4472,  0.1727,\n",
       "                       0.0311,  0.1005,  0.0598,  0.1126, -0.1863, -0.0337, -0.1716, -0.0921,\n",
       "                      -0.1091,  0.2521, -0.4810,  0.0456,  0.0056, -0.0235, -0.2933, -0.3052,\n",
       "                       0.0319, -0.1135, -0.3606, -0.0511, -0.0434,  0.3710,  0.1006,  0.1988,\n",
       "                      -0.0049, -0.0604,  0.0520,  0.0235, -0.0763,  0.2187, -0.1616,  0.0865,\n",
       "                      -0.3405,  0.1084,  0.0123, -0.2155, -0.0866, -0.4342, -0.1209, -0.0563,\n",
       "                       0.4384, -0.1127,  0.1204, -0.1247, -0.0800, -0.2166,  0.0425,  0.5024])),\n",
       "             ('bn2.running_var',\n",
       "              tensor([0.1874, 0.2180, 0.2439, 0.1782, 0.1760, 0.2357, 0.1997, 0.2242, 0.2053,\n",
       "                      0.2158, 0.1792, 0.1936, 0.1967, 0.1872, 0.1745, 0.2028, 0.2435, 0.1940,\n",
       "                      0.2075, 0.1948, 0.1986, 0.2011, 0.2215, 0.1833, 0.1743, 0.2463, 0.1844,\n",
       "                      0.2363, 0.2022, 0.1758, 0.2055, 0.2918, 0.1918, 0.1777, 0.1661, 0.2529,\n",
       "                      0.1762, 0.1775, 0.2146, 0.2214, 0.1765, 0.1858, 0.1816, 0.1793, 0.2269,\n",
       "                      0.2084, 0.1920, 0.2000, 0.1778, 0.1783, 0.1790, 0.2129, 0.2116, 0.1680,\n",
       "                      0.1724, 0.2163, 0.1605, 0.2373, 0.1788, 0.1984, 0.2097, 0.2040, 0.2208,\n",
       "                      0.1841, 0.1972, 0.1649, 0.2294, 0.2355, 0.1851, 0.1887, 0.1597, 0.2126,\n",
       "                      0.1934, 0.1811, 0.1757, 0.2043, 0.2688, 0.2595, 0.2238, 0.2194, 0.1887,\n",
       "                      0.1662, 0.2079, 0.2495, 0.1876, 0.1789, 0.1836, 0.1787, 0.1905, 0.2145,\n",
       "                      0.1893, 0.2311, 0.1961, 0.1787, 0.2272, 0.2013, 0.1913, 0.1737, 0.2100,\n",
       "                      0.2120, 0.1992, 0.2026, 0.1999, 0.1852, 0.1994, 0.1955, 0.1916, 0.2334,\n",
       "                      0.1830, 0.2252, 0.1579, 0.2024, 0.1686, 0.2132, 0.1927, 0.1871, 0.1782,\n",
       "                      0.2236, 0.1636, 0.2052, 0.1953, 0.1930, 0.2019, 0.1716, 0.2302, 0.2148,\n",
       "                      0.1920, 0.2125, 0.2264, 0.2210, 0.1844, 0.2545, 0.2269, 0.1893, 0.1857,\n",
       "                      0.1821, 0.1968, 0.1920, 0.1628, 0.1730, 0.1827, 0.2181, 0.2058, 0.1967,\n",
       "                      0.1816, 0.1991, 0.2015, 0.2511, 0.2390, 0.1998, 0.1850, 0.2049, 0.2248,\n",
       "                      0.2019, 0.2475, 0.2183, 0.1808, 0.2409, 0.2051, 0.2074, 0.2191, 0.2030,\n",
       "                      0.2188, 0.1819, 0.1676, 0.2058, 0.1773, 0.1946, 0.1836, 0.1800, 0.2971,\n",
       "                      0.2333, 0.2036, 0.2320, 0.1911, 0.2179, 0.1845, 0.2343, 0.2099, 0.1870,\n",
       "                      0.1859, 0.2113, 0.3057, 0.1622, 0.1600, 0.1999, 0.1955, 0.1956, 0.1994,\n",
       "                      0.2128, 0.2198, 0.1576, 0.1610, 0.1848, 0.1769, 0.1856, 0.1810, 0.2056,\n",
       "                      0.1965, 0.1902, 0.1978, 0.1824, 0.2066, 0.2146, 0.2091, 0.2043, 0.2473,\n",
       "                      0.2170, 0.1908, 0.1657, 0.2001, 0.2090, 0.1620, 0.1757, 0.1646, 0.1936,\n",
       "                      0.1841, 0.2028, 0.2302, 0.1858, 0.1835, 0.2088, 0.1832, 0.2364, 0.2059,\n",
       "                      0.1833, 0.1899, 0.1838, 0.1919, 0.2098, 0.2366, 0.1611, 0.1890, 0.1576,\n",
       "                      0.2068, 0.1904, 0.2119, 0.1843, 0.2072, 0.1953, 0.2464, 0.1524, 0.1736,\n",
       "                      0.2130, 0.2382, 0.2106, 0.1888, 0.2283, 0.2423, 0.1902, 0.2012, 0.2106,\n",
       "                      0.1656, 0.1977, 0.1939, 0.2006])),\n",
       "             ('bn2.num_batches_tracked', tensor(25006)),\n",
       "             ('fc3.weight',\n",
       "              tensor([[-0.0446,  0.0145,  0.0304,  ..., -0.0414,  0.0429, -0.0048],\n",
       "                      [ 0.0244,  0.0262, -0.0593,  ..., -0.0196,  0.0170,  0.0121],\n",
       "                      [ 0.0178,  0.0156, -0.0039,  ...,  0.0087, -0.0360, -0.0058],\n",
       "                      ...,\n",
       "                      [ 0.0519,  0.0101, -0.0598,  ..., -0.0282,  0.0328,  0.0054],\n",
       "                      [-0.0155,  0.0012,  0.0191,  ..., -0.0171,  0.0148, -0.0137],\n",
       "                      [ 0.0459, -0.0531, -0.0220,  ...,  0.0471, -0.0308,  0.0158]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([ 0.0507, -0.0166, -0.0141, -0.0348,  0.0146, -0.0594,  0.0518, -0.0256,\n",
       "                      -0.0073,  0.0545,  0.0166, -0.0359, -0.0235,  0.0572, -0.0524,  0.0016,\n",
       "                      -0.0063,  0.0255, -0.0475, -0.0222, -0.0108,  0.0426, -0.0567, -0.0245,\n",
       "                       0.0581,  0.0544,  0.0530, -0.0338,  0.0336,  0.0578, -0.0304,  0.0074,\n",
       "                      -0.0151, -0.0538, -0.0389,  0.0509, -0.0244,  0.0581, -0.0357, -0.0166,\n",
       "                      -0.0528, -0.0102, -0.0228, -0.0173, -0.0048,  0.0194, -0.0097,  0.0043,\n",
       "                      -0.0050, -0.0232, -0.0130,  0.0299, -0.0446, -0.0121, -0.0371,  0.0222,\n",
       "                      -0.0235, -0.0166, -0.0112, -0.0592,  0.0240, -0.0405,  0.0122, -0.0084])),\n",
       "             ('bn3.weight',\n",
       "              tensor([1.0822, 1.1368, 1.0287, 1.1335, 1.0156, 1.1270, 1.1132, 1.0203, 1.0234,\n",
       "                      1.1356, 1.0243, 1.0173, 1.1131, 0.9968, 1.1387, 1.1257, 1.1290, 1.0372,\n",
       "                      1.1478, 1.0389, 1.1343, 1.0380, 1.1189, 1.1393, 1.1353, 1.1163, 1.0703,\n",
       "                      1.1341, 1.1352, 1.1248, 1.0316, 1.1521, 1.0151, 1.1309, 1.1307, 1.0193,\n",
       "                      1.0275, 1.0223, 1.0100, 0.9866, 1.0710, 1.0835, 1.0291, 1.0003, 1.0049,\n",
       "                      1.1239, 1.0725, 1.1486, 1.0249, 1.0286, 0.9743, 1.0103, 1.1323, 1.0202,\n",
       "                      1.1449, 1.0121, 1.1244, 0.9746, 1.0118, 1.1271, 1.1082, 1.1350, 1.0202,\n",
       "                      1.0129])),\n",
       "             ('bn3.bias',\n",
       "              tensor([ 0.0906,  0.1346, -0.0255,  0.1309, -0.0349,  0.1445,  0.1238, -0.0368,\n",
       "                      -0.0412,  0.1351, -0.0418, -0.0335,  0.1285, -0.0510,  0.1266,  0.1252,\n",
       "                       0.1246, -0.0359,  0.1402, -0.0359,  0.1409, -0.0379,  0.1294,  0.1327,\n",
       "                       0.1414,  0.1252,  0.0770,  0.1349,  0.1328,  0.1314, -0.0386,  0.1320,\n",
       "                      -0.0389,  0.1348,  0.1344,  0.0099, -0.0473, -0.0368, -0.0431, -0.0575,\n",
       "                       0.0785,  0.1033, -0.0339, -0.0479, -0.0433,  0.1258,  0.0945,  0.1355,\n",
       "                      -0.0417, -0.0359, -0.0642, -0.0458,  0.1286, -0.0439,  0.1377, -0.0375,\n",
       "                       0.1363, -0.0513, -0.0512,  0.1319,  0.1159,  0.1271, -0.0465, -0.0466])),\n",
       "             ('bn3.running_mean',\n",
       "              tensor([ 0.3517, -0.1956,  0.1761, -0.3632,  0.0155, -0.4444, -0.2413,  0.3275,\n",
       "                       0.2272, -0.2544,  0.0814,  0.1922, -0.3908,  0.3776,  0.3055,  0.2417,\n",
       "                      -0.1562,  0.4635, -0.2972,  0.0395, -0.1298,  0.0423, -0.2587,  0.1831,\n",
       "                      -0.2944, -0.2399,  0.2542, -0.2453,  0.0958, -0.0408,  0.1048, -0.2576,\n",
       "                       0.1256,  0.0266, -0.3118,  0.4541,  0.1691,  0.1465, -0.0745,  0.0470,\n",
       "                      -0.2036, -0.5399,  0.0902,  0.1170, -0.0218, -0.2367, -0.1473, -0.1026,\n",
       "                       0.1466,  0.2542, -0.1224,  0.0643, -0.1277, -0.0164, -0.2303,  0.2168,\n",
       "                      -0.2049, -0.0693,  0.0009, -0.1711,  0.1458, -0.0640,  0.3650,  0.0429])),\n",
       "             ('bn3.running_var',\n",
       "              tensor([0.2997, 0.2168, 0.3078, 0.2202, 0.2458, 0.2038, 0.1881, 0.3011, 0.2740,\n",
       "                      0.3062, 0.3462, 0.3097, 0.1843, 0.3034, 0.2870, 0.2861, 0.2216, 0.3227,\n",
       "                      0.2677, 0.2912, 0.2658, 0.3161, 0.2067, 0.2490, 0.3403, 0.2257, 0.2381,\n",
       "                      0.2574, 0.2220, 0.2711, 0.2863, 0.2462, 0.3255, 0.1917, 0.2815, 0.2034,\n",
       "                      0.3109, 0.3328, 0.2684, 0.2232, 0.2177, 0.2013, 0.2604, 0.2989, 0.2829,\n",
       "                      0.1981, 0.1916, 0.2760, 0.2968, 0.2577, 0.2492, 0.2645, 0.2304, 0.3762,\n",
       "                      0.2403, 0.2955, 0.1954, 0.2372, 0.2701, 0.2229, 0.1830, 0.2746, 0.3353,\n",
       "                      0.3408])),\n",
       "             ('bn3.num_batches_tracked', tensor(25006)),\n",
       "             ('fc4.weight',\n",
       "              tensor([[-0.0837, -0.2181,  0.0679, -0.2565,  0.0527, -0.1129, -0.1251,  0.0866,\n",
       "                        0.0938, -0.2381,  0.0913,  0.0568, -0.1051,  0.0346, -0.2109, -0.1178,\n",
       "                       -0.1707,  0.1611, -0.2636,  0.1453, -0.1882,  0.1558, -0.1135, -0.2013,\n",
       "                       -0.1980, -0.1526, -0.0796, -0.2369, -0.2095, -0.2046,  0.1360, -0.2648,\n",
       "                        0.0870, -0.2040, -0.2402, -0.0324,  0.1366,  0.0749,  0.0670,  0.0330,\n",
       "                       -0.0790, -0.0874,  0.0848,  0.0691,  0.0672, -0.1660, -0.0787, -0.2495,\n",
       "                        0.1056,  0.1249,  0.0324,  0.0817, -0.2037,  0.1091, -0.2366,  0.0458,\n",
       "                       -0.1394,  0.0113,  0.1287, -0.1850, -0.1250, -0.2134,  0.1217,  0.0757]])),\n",
       "             ('fc4.bias', tensor([-0.2824]))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_Model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "vocab_size=50000\n",
    "embedding_dim=128\n",
    "max_length=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_model = LSTMModel(vocal_size=vocab_size, embedding_dim=embedding_dim,hidden_size=32, length=max_length, output_dim=1)\n",
    "predict_model.load_state_dict(torch.load('best_model_lstm_merge_acc0.78_mf10.37.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dự đoán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocess test with 3 type: ViCTSD, ViHSD, ViMergre\n",
    "def test_dataloader(dataset_test_type):\n",
    "    test = load_data(set_name='test', dataset=dataset_test_type)\n",
    "    test['label'] = test['label'].replace(2,1)\n",
    "    test['text'] = test['text'].apply(tokenize)\n",
    "    test_preprocess = preprocess_data(test,\n",
    "                                    url=True,\n",
    "                                    punctuation=True,\n",
    "                                    lowercase=True,\n",
    "                                    stopword=True,\n",
    "                                    special_stopwords=special_stopwords,\n",
    "                                    emoji=True)\n",
    "    X_test = test_preprocess['text'].astype(str)\n",
    "    y_test = test_preprocess['label']\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test = pad_sequences(X_test, maxlen=max_length, padding='post',truncating='post')\n",
    "    test_data = CustomDataset(X_test, y_test)\n",
    "    test_dataloader = DataLoader(dataset=test_data,\n",
    "                                    batch_sampler=StratifiedBatchSampler(torch.tensor(test_data.y_encoded), batch_size=BATCH_SIZE))\n",
    "    return test_dataloader\n",
    "\n",
    "vihsd_test = test_dataloader('vihsd')\n",
    "victsd_test = test_dataloader('victsd')\n",
    "vimerge_test = test_dataloader('merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def print_report(y_true, y_pred):\n",
    "    flat_y_pred_list = sum(y_pred, [])\n",
    "    flat_y_pred_list = sum(flat_y_pred_list, [])\n",
    "    flat_y_pred_list = [round(x) for x in flat_y_pred_list]\n",
    "    flat_y_real_list = sum(y_true,[])\n",
    "    print(classification_report(y_true=flat_y_real_list, y_pred=flat_y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: <torch.utils.data.dataloader.DataLoader object at 0x00000221D0F5C290>\n",
      "Test acc: 0.7677,   F1 test score: 0.4236, F1 test macro: 0.6690\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.91      6438\n",
      "           1       0.57      0.36      0.44      1242\n",
      "\n",
      "    accuracy                           0.85      7680\n",
      "   macro avg       0.73      0.66      0.68      7680\n",
      "weighted avg       0.83      0.85      0.84      7680\n",
      "\n",
      "Dataset: <torch.utils.data.dataloader.DataLoader object at 0x000002219445AD50>\n",
      "Test acc: 0.7570,   F1 test score: 0.4540, F1 test macro: 0.6839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91      5548\n",
      "           1       0.60      0.39      0.47      1132\n",
      "\n",
      "    accuracy                           0.85      6680\n",
      "   macro avg       0.74      0.67      0.69      6680\n",
      "weighted avg       0.84      0.85      0.84      6680\n",
      "\n",
      "Dataset: <torch.utils.data.dataloader.DataLoader object at 0x00000221C618F450>\n",
      "Test acc: 0.8453,   F1 test score: 0.0905, F1 test macro: 0.5044\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92       890\n",
      "           1       0.16      0.08      0.11       110\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.53      0.51      0.51      1000\n",
      "weighted avg       0.81      0.85      0.83      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test_loss, test_acc = 0,0\n",
    "# f1_test_pos_1 = []\n",
    "# f1_macro = []\n",
    "# predict_model.eval()\n",
    "# y_pred_list = []\n",
    "# y_real_list = []\n",
    "test_set = [vimerge_test, vihsd_test, victsd_test]\n",
    "for set in test_set:\n",
    "    test_loss, test_acc = 0,0\n",
    "    f1_test_pos_1 = []\n",
    "    f1_macro = []\n",
    "    predict_model.eval()\n",
    "    y_pred_list = []\n",
    "    y_real_list = []\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for batch, (X_test_, y_test_) in enumerate(set):\n",
    "        # 1. Forward pass\n",
    "        X_test_ = X_test_.long()\n",
    "        test_pred = predict_model(X_test_)\n",
    "\n",
    "        y_pred_list.append(test_pred.tolist())\n",
    "        y_real_list.append(y_test_.tolist())\n",
    "\n",
    "        # 2. Calculate the loss (accumulatively)\n",
    "        test_loss += loss_fn(test_pred.squeeze(dim=1), y_test_.float()).item()\n",
    "        # print(test_pred.shape)\n",
    "        # 3. Calculate accuracy\n",
    "        acc = accuracy_fn(y_true= y_test_.float(),\n",
    "                        y_pred = test_pred.squeeze(dim=1))\n",
    "        acc = (torch.round(test_pred)==y_test_).sum().item()/len(y_test_)\n",
    "        test_acc += acc\n",
    "        # 4. Calculate f1 score\n",
    "        pos_1, macro = f1_score_fn(y_true= y_test_.float(),\n",
    "                                y_pred = test_pred.squeeze(dim=1))\n",
    "        f1_test_pos_1.append(pos_1.item())\n",
    "        f1_macro.append(macro.item())\n",
    "    # Calculate the test loss average per batch\n",
    "    test_loss /= len(victsd_test)\n",
    "    # test_loss /= len(test_dataloader)\n",
    "    f1_test_pos_1_score = sum(f1_test_pos_1)/len(f1_test_pos_1)\n",
    "    f1_test_macro_score = sum(f1_macro)/len(f1_macro)\n",
    "    # Calculate the test acc average per batch\n",
    "    test_acc /= len(set)\n",
    "    print(f\"Dataset: {set}\\nTest acc: {test_acc:.4f},   F1 test score: {f1_test_pos_1_score:.4f}, F1 test macro: {f1_test_macro_score:.4f}\")\n",
    "    print_report(y_real_list, y_pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViCTSD: Test loss: 0.0188,   Test acc: 0.7065,   F1 test score: 0.1548, F1 test macro: 0.4883\n",
    "ViHSD: Test loss: 0.0131,   Test acc: 0.7261,   F1 test score: 0.4678, F1 test macro: 0.6828\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
