{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huấn luyện trên bộ dữ liệu ViHSD\n",
    "* Bao gồm dữ liệu thu thập từ mạng xã hội\n",
    "* Dữ liệu có tính toxic cao - phân biệt chủng tộc, vùng miền, công kích cá nhân, chửi đổng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvi\n",
    "from utility.utility import load_data\n",
    "import string\n",
    "import emoji_vietnamese  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup DATA for train and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_type = 'vihsd'\n",
    "train = load_data(set_name='train', dataset=dataset_train_type)\n",
    "dev = load_data(set_name='dev', dataset=dataset_train_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup DATA for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_type = 'vihsd'\n",
    "test = load_data(set_name='test', dataset=dataset_test_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'], dev['label'], test['label'] = train['label'].replace(2,1), dev['label'].replace(2,1), test['label'].replace(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "* Remove url in comment\n",
    "* remove punctuation\n",
    "* Lowercase data\n",
    "* Remove stopwords\n",
    "* Remove emoji\n",
    "* Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Thật tuyệt vời -> Thật tuyệt_vời\n",
    "    \"\"\"\n",
    "    return ViTokenizer.tokenize(text)\n",
    "\n",
    "# apply tokenize to text\n",
    "train['text'] = train['text'].apply(tokenize)\n",
    "dev['text'] = dev['text'].apply(tokenize)\n",
    "test['text'] = test['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    data,\n",
    "    url=True,\n",
    "    punctuation=True,\n",
    "    lowercase=True,\n",
    "    stopword=False,\n",
    "    special_stopwords=[],\n",
    "    emoji=False\n",
    "):\n",
    "    # Load stopwords\n",
    "    with open('./utility/Stopwords/vietnamese-stopwords-dash.txt', 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "    for word in special_stopwords:\n",
    "        stopwords.remove(word)\n",
    "    # Function to remove stopwords\n",
    "    def remove_stopwords(text):\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "        return ' '.join(words)\n",
    "    if url:\n",
    "        # Remove URLs\n",
    "        data['text'] = data['text'].str.replace(\n",
    "            r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "    if punctuation:\n",
    "        # Remove punctuation\n",
    "        data['text'] = data['text'].str.replace(\n",
    "            '['+string.punctuation+']', '', regex=True)\n",
    "    if lowercase:\n",
    "        # Lowercase\n",
    "        data['text'] = data['text'].str.lower()\n",
    "    if stopword:\n",
    "        # Remove stopword\n",
    "        data['text'] = data['text'].apply(remove_stopwords)\n",
    "    if emoji:\n",
    "        # Remove emojis\n",
    "        data['text'] = data['text'].apply(emoji_vietnamese.demojize)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_stopwords = [\"không\",\"không_có\",\"không_thể\",\"chưa\", \"được\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocess = preprocess_data(train,\n",
    "                                   url=True,\n",
    "                                   punctuation=True,\n",
    "                                   lowercase=True,\n",
    "                                   stopword=True,\n",
    "                                   special_stopwords=special_stopwords,\n",
    "                                   emoji=True)\n",
    "dev_preprocess = preprocess_data(dev,\n",
    "                                 url=True,\n",
    "                                 punctuation=True,\n",
    "                                 lowercase=True,\n",
    "                                 stopword=True,\n",
    "                                 special_stopwords=special_stopwords,\n",
    "                                 emoji=True)\n",
    "test_preprocess = preprocess_data(test,\n",
    "                                  url=True,\n",
    "                                  punctuation=True,\n",
    "                                  lowercase=True,\n",
    "                                  stopword=True,\n",
    "                                  special_stopwords=special_stopwords,\n",
    "                                  emoji=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['text'].astype(str)\n",
    "y_train = train['label']\n",
    "X_dev = dev['text'].astype(str)\n",
    "y_dev = dev['label']\n",
    "X_test = test['text'].astype(str)\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Để lấy dữ liệu cho nhanh chứ ko up lên git\n",
    "train_preprocess.to_csv('hsd_preprocessed_train_data.csv', index=False)\n",
    "dev_preprocess.to_csv('hsd_preprocessed_dev_data.csv', index=False)\n",
    "test_preprocess.to_csv('hsd_preprocessed_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from csv\n",
    "* Tokenizer and pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "vocab_size=50000\n",
    "embedding_dim=128\n",
    "max_length=128\n",
    "\n",
    "train_data = pd.read_csv('hsd_preprocessed_train_data.csv')\n",
    "dev_data = pd.read_csv('hsd_preprocessed_dev_data.csv')\n",
    "test_data = pd.read_csv('hsd_preprocessed_test_data.csv')\n",
    "\n",
    "# # Lấy 1 phần dữ liệu để chạy nhanh\n",
    "# train_data = train_data[:4454]\n",
    "# dev_data = dev_data[:527]\n",
    "\n",
    "# X_train, y_train,_,_ = train_test_split(train_data['text'], train_data['label'], test_size=0.99, random_state=42)\n",
    "# X_val, y_val,_,_ = train_test_split(dev_data['text'], dev_data['label'], test_size=0.99, random_state=42)\n",
    "# X_train= X_train.astype(str)\n",
    "# X_val= X_val.astype(str)\n",
    "\n",
    "X_train, y_train = train_data['text'].astype(str), train_data['label']\n",
    "X_dev, y_dev = dev_data['text'].astype(str), dev_data['label']\n",
    "X_test, y_test = test_data['text'].astype(str), test_data['label']\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='post',truncating='post')\n",
    "\n",
    "# Thực hiện thay đổi test để đưa vào tính toán val_acc\n",
    "X_dev = tokenizer.texts_to_sequences(X_dev)\n",
    "X_dev = pad_sequences(X_dev, maxlen=max_length, padding='post',truncating='post')\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "  y_pred_rounded = torch.round(y_pred)  \n",
    "  correct = torch.eq(y_true, y_pred_rounded).sum().item()\n",
    "  acc = (correct/len(y_pred))*100\n",
    "  return acc\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "def f1_score_fn(y_true, y_pred):\n",
    "  y_true = y_true.int().tolist()\n",
    "  y_pred = torch.round(y_pred).int().tolist()\n",
    "  f1_score_pos1 = f1_score(y_true=y_true, y_pred=y_pred, pos_label=1)\n",
    "  f1_score_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "  # print(classification_report(y_true=y_true, y_pred=y_pred,zero_division=1))  \n",
    "  return f1_score_pos1, f1_score_macro_average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_encoded: np.ndarray, y_encoded: pd.core.series.Series):\n",
    "        # Setup\n",
    "        self.x_encoded = x_encoded\n",
    "        self.y_encoded = y_encoded.tolist()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.FloatTensor(self.x_encoded[idx]), self.y_encoded[idx])\n",
    "        # return (self.x_encoded[idx], self.y_encoded[idx])\n",
    "        # return {'text': self.x[idx], 'label': self.y_encoded[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x_encoded.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(X_train, y_train)\n",
    "dev_data = CustomDataset(X_dev, y_dev)\n",
    "test_data = CustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class StratifiedBatchSampler:\n",
    "    \"\"\"Stratified batch sampling\n",
    "    Provides equal representation of target classes in each batch\n",
    "    \"\"\"\n",
    "    def __init__(self, y, batch_size, shuffle=True):\n",
    "        if torch.is_tensor(y):\n",
    "            y = y.numpy()\n",
    "        assert len(y.shape) == 1, 'label array must be 1D'\n",
    "        n_batches = int(len(y) / batch_size)\n",
    "        self.skf = StratifiedKFold(n_splits=n_batches, shuffle=shuffle)\n",
    "        self.X = torch.randn(len(y),1).numpy()\n",
    "        self.y = y\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.skf.random_state = torch.randint(0,int(1e8),size=()).item()\n",
    "        for train_idx, test_idx in self.skf.split(self.X, self.y):\n",
    "            yield test_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE=64\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_sampler=StratifiedBatchSampler(torch.tensor(train_data.y_encoded), batch_size=BATCH_SIZE))\n",
    "dev_dataloader = DataLoader(dataset=dev_data,\n",
    "                              batch_sampler=StratifiedBatchSampler(torch.tensor(dev_data.y_encoded), batch_size=BATCH_SIZE))                            \n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                                batch_sampler=StratifiedBatchSampler(torch.tensor(test_data.y_encoded), batch_size=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocal_size, embedding_dim, hidden_size, output_dim, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=vocal_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "        self.dropout_lstm = nn.Dropout(p=0.2) \n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=hidden_size*2, out_features=16)  # Kết nối trực tiếp LSTM với lớp fully connected \n",
    "        self.ln1 = nn.LayerNorm(16)  \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=16, out_features=output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout_lstm(x) \n",
    "        x = x[:, -1, :] # Lấy output của timestep cuối cùng\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.ln1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embed): Embedding(50000, 128)\n",
       "  (lstm): LSTM(128, 32, batch_first=True, bidirectional=True)\n",
       "  (dropout_lstm): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=64, out_features=16, bias=True)\n",
       "  (ln1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_Model = LSTMModel(vocal_size=vocab_size, embedding_dim=embedding_dim,hidden_size=32, output_dim=1, num_layers=2)\n",
    "LSTM_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 18582, 1: 3960})\n"
     ]
    }
   ],
   "source": [
    "def weighted_binary_cross_entropy(y_true, y_pred, pos_weight):\n",
    "    \"\"\"\n",
    "    Weighted Binary Cross Entropy (WBCE) = - (w * y * log(p) + (1 - y) * log(1 - p))\n",
    "    Trong đó:\n",
    "    y: Nhãn thực tế (0 hoặc 1).\n",
    "    p: Xác suất dự đoán cho lớp positive (nhãn 1).\n",
    "    w: Trọng số cho lớp positive.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-7\n",
    "    y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)  # giới hạn giá trị dự đoán trong khoảng (epsilon, 1 - epsilon)\n",
    "    bce = - (pos_weight * y_true * torch.log(y_pred) +\n",
    "              (1 - y_true) * torch.log(1 - y_pred))     # Binary Cross Entropy trong đo tăng tầm quan trọng khi dự đoán sai lớp 1\n",
    "    return torch.mean(bce)\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(y_train))\n",
    "neg_count = Counter(y_train)[0]\n",
    "pos_count = Counter(y_train)[1]\n",
    "pos_weight = torch.tensor((neg_count) / pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(LSTM_Model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fd276a80b84c6291fa0f669971b82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 70\u001b[0m\n\u001b[0;32m     68\u001b[0m LSTM_Model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m---> 70\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m batch, (X_test_, y_test_) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dev_dataloader):\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# 1. Forward pass\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     X_test_ \u001b[38;5;241m=\u001b[39m X_test_\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     73\u001b[0m     test_pred \u001b[38;5;241m=\u001b[39m LSTM_Model(X_test_)\n",
      "Cell \u001b[1;32mIn[96], line 70\u001b[0m\n\u001b[0;32m     68\u001b[0m LSTM_Model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m---> 70\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m batch, (X_test_, y_test_) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dev_dataloader):\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# 1. Forward pass\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     X_test_ \u001b[38;5;241m=\u001b[39m X_test_\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     73\u001b[0m     test_pred \u001b[38;5;241m=\u001b[39m LSTM_Model(X_test_)\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Write a training and evaluationg loop for model_1\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Train for longer\n",
    "epochs = 15\n",
    "\n",
    "# # Put data on the target device\n",
    "# X_padded_sequences, y_train = torch.tensor(X_padded_sequences).to(device), torch.tensor(y_train).to(device)\n",
    "# padded_val_sequences, y_test=  torch.tensor(padded_val_sequences).to(device), torch.tensor(y_test).to(device)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "best_f1_score = 0\n",
    "best_acc_score = 0\n",
    "\n",
    "# Create training and test loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  print(f\"Epoch: {epoch}\\n------\")\n",
    "  ### Training\n",
    "  train_loss, train_acc=0,0\n",
    "  f1_train_pos_1 = []\n",
    "  f1_train_macro = []\n",
    "  cnt = 0\n",
    "  f1_score_list = []\n",
    "  LSTM_Model.train()\n",
    "  # Add a loop to loop through the training batches\n",
    "  for batch, (X, y) in enumerate(train_dataloader):\n",
    "    cnt+=1\n",
    "    # 1. Forward\n",
    "    X = X.long()\n",
    "    y_pred = LSTM_Model(X)\n",
    "\n",
    "    # 2. Calculate the loss\n",
    "    # loss = loss_fn(y_pred.squeeze(), y.float().squeeze())\n",
    "    loss = weighted_binary_cross_entropy(y_true=y.float().squeeze(), y_pred=y_pred.squeeze(), pos_weight=pos_weight)\n",
    "    train_loss += loss.item()\n",
    "    f1_pos_1, f1_macro = f1_score_fn(y_true= y.float(),\n",
    "                            y_pred = y_pred.squeeze(dim=1))\n",
    "    f1_train_pos_1.append(f1_pos_1.item())\n",
    "    f1_train_macro.append(f1_macro.item())\n",
    "\n",
    "    # 3.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4.\n",
    "    loss.backward()\n",
    "\n",
    "    # 5.\n",
    "    optimizer.step()\n",
    "\n",
    "    # 6. Calculate accuracy metric\n",
    "    y_pred_class = torch.round(y_pred)\n",
    "    train_acc += (y_pred_class==y).sum().item()/len(y_pred_class)\n",
    "\n",
    "  # Divide total train loss by length of train dataloader\n",
    "  # train_loss\n",
    "  train_loss /= len(train_dataloader)\n",
    "  train_acc /= len(train_dataloader)\n",
    "  f1_train_pos_1 = sum(f1_train_pos_1)/len(f1_train_pos_1)\n",
    "  f1_train_macro = sum(f1_train_macro)/len(f1_train_macro)\n",
    "\n",
    "  ### Testing\n",
    "  test_loss, test_acc = 0,0\n",
    "  f1_test_pos_1 = []\n",
    "  f1_test_macro = []\n",
    "  LSTM_Model.eval()\n",
    "  with torch.inference_mode():\n",
    "    for batch, (X_test_, y_test_) in enumerate(dev_dataloader):\n",
    "      # 1. Forward pass\n",
    "      X_test_ = X_test_.long()\n",
    "      test_pred = LSTM_Model(X_test_)\n",
    "\n",
    "      # 2. Calculate the loss (accumulatively)\n",
    "      test_loss += weighted_binary_cross_entropy(\n",
    "                y_test_.float(), test_pred.squeeze(dim=1), pos_weight)\n",
    "      # print(test_pred.shape)\n",
    "      # 3. Calculate accuracy\n",
    "      acc = accuracy_fn(y_true= y_test_.float(),\n",
    "                        y_pred = test_pred.squeeze(dim=1))\n",
    "      acc = (torch.round(test_pred)==y_test_).sum().item()/len(y_test_)\n",
    "      test_acc += acc\n",
    "      # 4. Calculate f1 score\n",
    "      pos_1, macro = f1_score_fn(y_true= y_test_.float(),\n",
    "                              y_pred = test_pred.squeeze(dim=1))\n",
    "      f1_test_pos_1.append(pos_1.item())\n",
    "      f1_test_macro.append(macro.item())\n",
    "\n",
    "    # Calculate the test loss average per batch\n",
    "    test_loss /= len(dev_dataloader)\n",
    "    # test_loss /= len(test_dataloader)\n",
    "    f1_test_pos_1 = sum(f1_test_pos_1)/len(f1_test_pos_1)\n",
    "    f1_test_macro = sum(f1_test_macro)/len(f1_test_macro)\n",
    "    # Calculate the test acc average per batch\n",
    "    test_acc /= len(dev_dataloader)\n",
    "\n",
    "  # print out what happen\n",
    "  print(f\"\\nTrain loss: {train_loss:.4f}, Train acc: {train_acc:.4f}, F1 train pos 1 score: {f1_train_pos_1:.4f}, F1 train macro score: {f1_train_macro:.4f} | \\nTest loss: {test_loss:.4f},   Test acc: {test_acc:.4f},   F1 test pos 1 score: {f1_test_pos_1:.4f}, F1 test macro scoreL {f1_test_macro:.4f}\")\n",
    "  # print(f\"\\nTrain loss; {train_loss:.4f}\")\n",
    "\n",
    "  # Lưu trữ trọng số mô hình tốt nhất\n",
    "  if f1_test_pos_1 > best_f1_score-0.03 and test_acc > best_acc_score-0.03 and f1_test_pos_1 + test_acc > best_f1_score+best_acc_score:\n",
    "    best_f1_score = f1_test_pos_1\n",
    "    best_acc_score = test_acc\n",
    "    torch.save(LSTM_Model.state_dict(), \"best_model_bilstm_vihsd.pth\")\n",
    "    print(\"save model at this epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lưu model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embed.weight',\n",
       "              tensor([[ 0.0041, -0.0737, -0.0539,  ..., -1.0554, -1.1585, -0.3546],\n",
       "                      [ 0.6536, -0.2913,  1.2105,  ..., -1.5105, -0.1381,  1.5027],\n",
       "                      [-0.4788,  0.5766,  0.4390,  ...,  0.9184,  0.5358, -0.0737],\n",
       "                      ...,\n",
       "                      [ 0.0737,  0.6009,  1.6344,  ..., -1.4264,  0.8952,  0.6773],\n",
       "                      [ 0.1097,  0.3854, -1.3889,  ...,  1.2902,  1.0346, -0.0775],\n",
       "                      [-0.3203, -0.0348, -1.3883,  ...,  0.7865, -0.3805, -0.0595]])),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[-0.0903, -0.1114,  0.0563,  ..., -0.0597, -0.0293, -0.0291],\n",
       "                      [ 0.1450, -0.0268, -0.1452,  ..., -0.0335, -0.1432, -0.1107],\n",
       "                      [-0.0587, -0.1304,  0.0320,  ..., -0.0369, -0.0744, -0.1328],\n",
       "                      ...,\n",
       "                      [ 0.0530,  0.0588, -0.1308,  ...,  0.0297,  0.0302, -0.0578],\n",
       "                      [-0.1676, -0.0049, -0.1298,  ...,  0.1150, -0.0065, -0.1270],\n",
       "                      [-0.1356,  0.0349, -0.0267,  ..., -0.0575, -0.0367,  0.1067]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[-0.1278, -0.0257,  0.1750,  ..., -0.1019,  0.0556, -0.0801],\n",
       "                      [ 0.0314, -0.1611,  0.1309,  ...,  0.1542,  0.0277,  0.0407],\n",
       "                      [-0.0410,  0.1095,  0.0111,  ..., -0.0213,  0.1187, -0.0739],\n",
       "                      ...,\n",
       "                      [ 0.0653, -0.0581, -0.1545,  ..., -0.1078, -0.0365, -0.1348],\n",
       "                      [ 0.1572,  0.0028, -0.1746,  ..., -0.0442,  0.0572,  0.0470],\n",
       "                      [-0.0181,  0.1312, -0.0080,  ...,  0.0192, -0.0755, -0.0990]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([-0.1224, -0.0197, -0.0960, -0.0341,  0.1476,  0.1171,  0.0591,  0.1159,\n",
       "                      -0.1364,  0.0207,  0.0126,  0.0455,  0.0549,  0.0801,  0.0824, -0.1583,\n",
       "                       0.1127, -0.0717,  0.0454, -0.0721, -0.0929, -0.1122,  0.0240,  0.0758,\n",
       "                       0.0955,  0.1102, -0.1082,  0.0522,  0.1033, -0.1613, -0.0735,  0.0805,\n",
       "                      -0.1399,  0.0126, -0.0846,  0.0664, -0.1193,  0.1477, -0.1456, -0.1521,\n",
       "                      -0.0438, -0.0316, -0.1859,  0.0984, -0.0134, -0.1365, -0.1745,  0.0399,\n",
       "                      -0.1024,  0.1618, -0.0975, -0.0862, -0.0398, -0.0061, -0.1778,  0.0536,\n",
       "                       0.1103,  0.0242,  0.0992, -0.0652,  0.1164, -0.1283,  0.1526, -0.1260,\n",
       "                       0.0369, -0.0047, -0.0783,  0.0005,  0.0751, -0.0711,  0.1482,  0.1288,\n",
       "                      -0.0026,  0.1347,  0.0431,  0.0024, -0.0562,  0.0350, -0.0039,  0.0666,\n",
       "                      -0.0828, -0.0863, -0.0598, -0.1365, -0.0711, -0.0247, -0.0178, -0.1530,\n",
       "                      -0.0722,  0.0631,  0.0201,  0.1408, -0.1456, -0.1322,  0.0903,  0.1120,\n",
       "                      -0.0562,  0.1613,  0.0937, -0.0227, -0.1192, -0.0721, -0.0905, -0.1260,\n",
       "                       0.1449, -0.1611,  0.1531,  0.1031,  0.1110,  0.1248,  0.0737,  0.1388,\n",
       "                       0.0896,  0.0299,  0.1230,  0.1381,  0.1055, -0.1082,  0.0730, -0.1224,\n",
       "                       0.0859, -0.1642, -0.0472,  0.0235, -0.0261,  0.1430,  0.1769, -0.1302])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([-0.1121,  0.0793,  0.1020, -0.1008, -0.1515,  0.1624,  0.1786,  0.1852,\n",
       "                      -0.1006, -0.0071, -0.0284,  0.1500,  0.0120, -0.1250, -0.1230, -0.1189,\n",
       "                       0.1458, -0.0801, -0.0244, -0.0971,  0.0104,  0.0007, -0.1301,  0.0836,\n",
       "                      -0.0239, -0.1062, -0.0795, -0.0135,  0.0981,  0.0909,  0.0991, -0.0229,\n",
       "                      -0.0150,  0.0622, -0.0412, -0.0714,  0.1317, -0.1290,  0.1443,  0.0738,\n",
       "                       0.0336,  0.0609, -0.1347, -0.1585, -0.0926,  0.1069, -0.1279,  0.1723,\n",
       "                      -0.1059,  0.0427,  0.0057, -0.1410,  0.1416,  0.0803,  0.0131, -0.1411,\n",
       "                      -0.1553, -0.0389, -0.0801,  0.0895,  0.1331, -0.0478, -0.1149, -0.0677,\n",
       "                      -0.0429, -0.0191, -0.1681,  0.1576, -0.1261,  0.1378, -0.1602,  0.1056,\n",
       "                       0.1438,  0.1287,  0.1095, -0.0128,  0.1752, -0.0769, -0.1300, -0.1055,\n",
       "                      -0.1231, -0.0421, -0.1464,  0.1325, -0.1405,  0.0331, -0.1183,  0.0376,\n",
       "                       0.1288,  0.1270,  0.1516, -0.0826, -0.0161, -0.0677,  0.1181, -0.0891,\n",
       "                      -0.1635,  0.1669, -0.0172, -0.0419,  0.0574,  0.0924, -0.1283, -0.0133,\n",
       "                       0.1347, -0.0406, -0.0846,  0.0422, -0.1394,  0.0214, -0.1009,  0.0567,\n",
       "                       0.1024,  0.1234, -0.0054, -0.0744,  0.1575,  0.1757,  0.1294, -0.0270,\n",
       "                       0.1383, -0.0213,  0.0232,  0.1461,  0.0959, -0.0288,  0.1695,  0.1708])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[ 0.0090,  0.0151,  0.0056,  ..., -0.0059,  0.0059,  0.0009],\n",
       "                      [-0.0130, -0.0070, -0.0137,  ..., -0.0129, -0.0032,  0.0130],\n",
       "                      [ 0.0015,  0.0027, -0.0185,  ...,  0.0094,  0.0058, -0.0154],\n",
       "                      ...,\n",
       "                      [-0.0129,  0.0155,  0.0101,  ..., -0.0104,  0.0082, -0.0034],\n",
       "                      [-0.0108,  0.0171, -0.0118,  ...,  0.0091, -0.0027, -0.0110],\n",
       "                      [-0.0154, -0.0065, -0.0085,  ...,  0.0019,  0.0130,  0.0100]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.0088, -0.0084, -0.0081,  ..., -0.0093,  0.0016,  0.0084])),\n",
       "             ('bn1.weight',\n",
       "              tensor([1.0007, 0.9977, 1.0013,  ..., 0.9955, 1.0001, 0.9980])),\n",
       "             ('bn1.bias',\n",
       "              tensor([ 7.6082e-05, -8.8174e-04,  8.7482e-04,  ..., -3.8555e-03,\n",
       "                       1.9863e-04,  6.2439e-04])),\n",
       "             ('bn1.running_mean',\n",
       "              tensor([-0.0474, -0.0012,  0.0011,  ..., -0.0090, -0.0008, -0.0780])),\n",
       "             ('bn1.running_var',\n",
       "              tensor([0.0017, 0.0014, 0.0016,  ..., 0.0015, 0.0016, 0.0015])),\n",
       "             ('bn1.num_batches_tracked', tensor(3150)),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.0023, -0.0054, -0.0009,  ...,  0.0141, -0.0054, -0.0153],\n",
       "                      [ 0.0070,  0.0307, -0.0114,  ..., -0.0027, -0.0059,  0.0212],\n",
       "                      [ 0.0273, -0.0173, -0.0038,  ..., -0.0004, -0.0191,  0.0171],\n",
       "                      ...,\n",
       "                      [ 0.0003,  0.0285,  0.0239,  ...,  0.0210,  0.0152, -0.0302],\n",
       "                      [-0.0051,  0.0153, -0.0117,  ..., -0.0070,  0.0032, -0.0091],\n",
       "                      [ 0.0314,  0.0145, -0.0134,  ..., -0.0186, -0.0294,  0.0133]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.0105, -0.0291,  0.0048, -0.0048,  0.0280, -0.0143,  0.0063,  0.0044,\n",
       "                      -0.0134,  0.0187,  0.0144,  0.0248,  0.0128,  0.0196, -0.0013, -0.0281,\n",
       "                      -0.0238, -0.0008,  0.0116, -0.0287, -0.0262,  0.0163,  0.0259, -0.0126,\n",
       "                      -0.0118,  0.0287,  0.0247, -0.0003, -0.0134,  0.0256,  0.0021,  0.0051,\n",
       "                      -0.0111, -0.0193, -0.0213, -0.0268, -0.0201,  0.0101, -0.0254, -0.0034,\n",
       "                      -0.0052, -0.0117,  0.0254,  0.0075,  0.0028,  0.0143,  0.0137,  0.0147,\n",
       "                       0.0258, -0.0196, -0.0143,  0.0227,  0.0098, -0.0061,  0.0172, -0.0048,\n",
       "                       0.0238, -0.0057,  0.0080, -0.0181,  0.0117, -0.0011, -0.0093, -0.0119,\n",
       "                       0.0248, -0.0227,  0.0069, -0.0158, -0.0289,  0.0017,  0.0086, -0.0233,\n",
       "                       0.0249, -0.0180,  0.0286,  0.0041,  0.0103, -0.0295, -0.0145,  0.0077,\n",
       "                       0.0203,  0.0059,  0.0243,  0.0207,  0.0141,  0.0180, -0.0225,  0.0257,\n",
       "                      -0.0239, -0.0124,  0.0079,  0.0244, -0.0075, -0.0147, -0.0072,  0.0175,\n",
       "                       0.0189,  0.0243, -0.0151, -0.0261,  0.0005, -0.0297, -0.0080,  0.0263,\n",
       "                       0.0023,  0.0310,  0.0300, -0.0072, -0.0089,  0.0230, -0.0155,  0.0163,\n",
       "                       0.0261,  0.0142,  0.0082,  0.0237,  0.0152, -0.0171,  0.0228,  0.0203,\n",
       "                       0.0055, -0.0207,  0.0081,  0.0072, -0.0225, -0.0227,  0.0127, -0.0073,\n",
       "                      -0.0070,  0.0066,  0.0042, -0.0033, -0.0018,  0.0063,  0.0115,  0.0210,\n",
       "                       0.0225,  0.0132, -0.0053,  0.0097, -0.0236, -0.0083, -0.0122,  0.0023,\n",
       "                      -0.0118, -0.0296,  0.0217, -0.0071,  0.0144, -0.0103,  0.0187, -0.0210,\n",
       "                       0.0048, -0.0214,  0.0196,  0.0038,  0.0010, -0.0200, -0.0074, -0.0036,\n",
       "                       0.0234,  0.0081,  0.0196, -0.0102, -0.0246, -0.0157, -0.0018,  0.0002,\n",
       "                       0.0102,  0.0060, -0.0146,  0.0174, -0.0062, -0.0014,  0.0077, -0.0110,\n",
       "                      -0.0115,  0.0116,  0.0161, -0.0132,  0.0164,  0.0231, -0.0120,  0.0302,\n",
       "                      -0.0056,  0.0246,  0.0263, -0.0041,  0.0259,  0.0286, -0.0090,  0.0053,\n",
       "                      -0.0228,  0.0152,  0.0256, -0.0292,  0.0113, -0.0126, -0.0225, -0.0194,\n",
       "                      -0.0170,  0.0025, -0.0248, -0.0070,  0.0156, -0.0167,  0.0124, -0.0196,\n",
       "                      -0.0279,  0.0023,  0.0080, -0.0160, -0.0137,  0.0028, -0.0064,  0.0049,\n",
       "                       0.0051, -0.0289,  0.0251,  0.0169,  0.0041, -0.0294, -0.0051,  0.0068,\n",
       "                      -0.0206,  0.0108,  0.0056,  0.0189, -0.0155, -0.0226,  0.0033, -0.0074,\n",
       "                       0.0196, -0.0095, -0.0280, -0.0184,  0.0174, -0.0213, -0.0207,  0.0139,\n",
       "                      -0.0039,  0.0124,  0.0043, -0.0254,  0.0268, -0.0091,  0.0175, -0.0143,\n",
       "                       0.0123,  0.0156,  0.0123,  0.0126, -0.0252, -0.0089,  0.0034,  0.0105])),\n",
       "             ('bn2.weight',\n",
       "              tensor([0.9992, 1.0007, 0.9965, 0.9995, 0.9992, 0.9975, 1.0020, 1.0015, 0.9994,\n",
       "                      0.9976, 0.9984, 1.0056, 1.0060, 1.0007, 0.9986, 0.9987, 1.0019, 0.9990,\n",
       "                      0.9985, 1.0025, 1.0041, 0.9971, 1.0096, 1.0020, 0.9996, 0.9939, 0.9990,\n",
       "                      0.9979, 0.9977, 1.0021, 1.0020, 0.9953, 0.9974, 0.9984, 1.0048, 0.9963,\n",
       "                      0.9961, 0.9936, 0.9995, 0.9962, 1.0004, 0.9983, 0.9984, 0.9946, 0.9989,\n",
       "                      0.9980, 0.9977, 0.9986, 1.0004, 1.0020, 1.0008, 1.0027, 0.9979, 0.9995,\n",
       "                      0.9994, 1.0008, 0.9995, 1.0031, 1.0023, 0.9976, 0.9966, 0.9989, 0.9988,\n",
       "                      1.0009, 0.9961, 0.9971, 0.9989, 0.9952, 0.9970, 0.9974, 0.9974, 0.9954,\n",
       "                      1.0037, 0.9965, 0.9967, 1.0113, 0.9970, 1.0002, 0.9920, 1.0033, 1.0006,\n",
       "                      1.0002, 1.0070, 0.9959, 1.0002, 1.0005, 1.0040, 0.9990, 0.9990, 1.0002,\n",
       "                      1.0010, 0.9969, 0.9997, 1.0016, 0.9966, 0.9982, 1.0012, 0.9971, 0.9999,\n",
       "                      1.0063, 0.9955, 1.0035, 1.0004, 0.9957, 0.9996, 0.9966, 0.9989, 1.0003,\n",
       "                      0.9999, 1.0030, 1.0066, 0.9973, 1.0000, 0.9978, 1.0012, 0.9957, 0.9970,\n",
       "                      1.0012, 0.9981, 1.0001, 0.9978, 0.9957, 1.0000, 0.9967, 1.0067, 1.0011,\n",
       "                      0.9968, 1.0014, 1.0004, 1.0002, 1.0009, 0.9964, 1.0014, 0.9994, 0.9983,\n",
       "                      0.9966, 0.9986, 1.0021, 0.9972, 0.9990, 1.0019, 0.9965, 1.0008, 0.9988,\n",
       "                      0.9969, 0.9991, 0.9994, 0.9963, 0.9951, 1.0041, 1.0070, 0.9984, 1.0001,\n",
       "                      1.0016, 0.9984, 1.0013, 1.0069, 0.9983, 0.9968, 1.0009, 1.0090, 0.9984,\n",
       "                      1.0017, 1.0023, 0.9967, 0.9968, 1.0031, 0.9972, 0.9991, 1.0090, 0.9974,\n",
       "                      0.9997, 0.9954, 0.9971, 1.0006, 0.9996, 1.0021, 0.9990, 1.0014, 0.9993,\n",
       "                      0.9991, 0.9928, 0.9950, 1.0007, 1.0001, 1.0141, 0.9970, 1.0012, 0.9993,\n",
       "                      0.9960, 0.9969, 0.9978, 0.9994, 0.9990, 0.9988, 0.9952, 1.0019, 1.0091,\n",
       "                      1.0013, 0.9976, 0.9964, 1.0010, 1.0000, 1.0001, 1.0007, 0.9989, 0.9981,\n",
       "                      0.9966, 0.9977, 1.0082, 0.9973, 0.9989, 0.9967, 0.9983, 1.0011, 1.0001,\n",
       "                      0.9978, 0.9971, 0.9963, 0.9968, 0.9996, 0.9973, 1.0053, 0.9998, 0.9979,\n",
       "                      1.0009, 1.0065, 0.9984, 0.9994, 1.0001, 0.9971, 0.9984, 1.0014, 1.0010,\n",
       "                      1.0005, 0.9981, 0.9968, 0.9963, 0.9962, 0.9981, 0.9999, 0.9977, 0.9997,\n",
       "                      0.9983, 0.9991, 1.0009, 1.0005, 1.0023, 0.9964, 0.9965, 1.0083, 1.0013,\n",
       "                      0.9966, 1.0005, 0.9977, 0.9994])),\n",
       "             ('bn2.bias',\n",
       "              tensor([-1.9327e-03,  8.8252e-04, -2.3972e-03,  1.1834e-03, -1.2238e-04,\n",
       "                      -3.8871e-04,  3.0071e-03, -7.6854e-04, -4.0088e-04, -2.1730e-03,\n",
       "                      -6.9715e-04, -4.3452e-04,  2.7214e-04,  5.7170e-04, -9.6600e-04,\n",
       "                      -7.6247e-04,  3.7994e-05, -8.8976e-04, -2.2356e-03, -2.4220e-03,\n",
       "                      -1.0598e-04, -7.8058e-04,  3.4986e-04,  2.2756e-03,  8.8180e-04,\n",
       "                      -2.9133e-03, -5.3701e-05, -2.3007e-03, -1.1249e-03,  2.8447e-03,\n",
       "                       1.4292e-03, -2.4373e-03, -1.6296e-03, -1.8892e-03, -1.6999e-04,\n",
       "                      -1.4053e-03, -4.1123e-03, -3.6420e-03, -1.3755e-04, -1.6616e-03,\n",
       "                      -4.3841e-04, -9.8609e-04, -2.7890e-04, -2.1111e-03, -9.3347e-04,\n",
       "                       1.1718e-04, -1.5633e-03, -3.8091e-04,  9.1401e-05, -2.5370e-04,\n",
       "                      -9.7989e-04,  6.3877e-04, -1.2124e-03, -2.3319e-04,  5.6806e-04,\n",
       "                       1.2676e-03, -4.3665e-04, -3.9332e-06,  1.5804e-03, -2.4082e-04,\n",
       "                      -1.4863e-03, -1.1999e-03, -1.0209e-03,  1.6141e-03, -1.1003e-03,\n",
       "                      -1.9218e-03, -2.9830e-04, -3.0357e-03, -6.5199e-04, -1.7296e-03,\n",
       "                      -1.3766e-03, -2.6464e-03, -9.3993e-04, -2.9608e-03, -1.6550e-03,\n",
       "                       1.4191e-03, -1.7105e-03,  6.5780e-04, -6.0798e-03,  4.2664e-04,\n",
       "                       1.4963e-03, -6.7429e-04, -6.1729e-04, -1.5960e-03, -4.9681e-05,\n",
       "                       8.2532e-04,  3.3111e-03, -4.3470e-04, -1.0812e-03,  6.3004e-04,\n",
       "                       1.1692e-03, -1.0104e-03,  6.9404e-04,  2.4611e-03, -3.3357e-03,\n",
       "                      -6.8315e-04,  1.7467e-03, -1.7596e-03, -7.2276e-05,  1.0433e-03,\n",
       "                      -3.4559e-03, -9.2036e-04,  2.6132e-03, -2.5175e-03,  3.8304e-04,\n",
       "                      -9.3194e-04, -1.0399e-03,  3.1168e-04,  1.7153e-03, -8.7616e-05,\n",
       "                      -3.8466e-04, -2.7595e-03,  2.4729e-04, -3.3833e-03, -1.1530e-03,\n",
       "                      -1.5959e-03, -3.6122e-04,  1.7841e-03, -2.5884e-03,  2.0837e-03,\n",
       "                      -2.1717e-03, -2.8116e-03,  1.3943e-03, -2.5083e-03,  3.8985e-03,\n",
       "                       1.1258e-03, -2.6886e-03,  2.1148e-03, -7.8224e-05,  1.5925e-03,\n",
       "                      -1.1861e-03, -2.2861e-03,  2.0945e-03, -3.3856e-04, -1.5759e-03,\n",
       "                      -1.7939e-03, -7.2863e-04,  1.7928e-04, -1.1661e-03, -6.0236e-04,\n",
       "                      -2.1306e-03, -1.3552e-03,  2.6254e-04, -1.5865e-03, -1.6257e-03,\n",
       "                      -6.4108e-04,  8.8656e-04, -2.8139e-03, -3.8509e-03,  9.7786e-04,\n",
       "                       1.8775e-03, -9.2434e-04,  9.0508e-04,  1.6727e-03,  3.8970e-04,\n",
       "                       1.2513e-03,  2.7993e-03, -6.6430e-04, -1.8109e-03,  7.5246e-04,\n",
       "                       1.7921e-03,  5.8104e-04,  2.2638e-03,  2.4814e-03, -2.1915e-03,\n",
       "                      -2.9528e-03,  8.4432e-04, -2.1125e-03, -5.3534e-04,  9.0635e-04,\n",
       "                      -2.0969e-03, -5.2403e-04, -4.1438e-03, -1.1133e-03, -1.7114e-03,\n",
       "                       1.9167e-03,  4.2450e-03, -2.7435e-04, -2.9991e-04, -6.7365e-04,\n",
       "                      -3.1157e-04, -4.8443e-03, -2.4556e-03,  3.2232e-03, -9.6509e-04,\n",
       "                       1.4591e-03, -2.3568e-03,  1.0955e-03,  1.1633e-03, -3.4668e-03,\n",
       "                      -2.2019e-03, -1.0247e-03, -6.5516e-04, -4.3639e-04, -2.9293e-04,\n",
       "                      -4.4206e-03,  2.2908e-03, -1.0718e-04, -2.1698e-03, -6.5091e-04,\n",
       "                      -5.3107e-03,  1.9694e-03, -6.8107e-04,  1.2330e-03,  2.1301e-03,\n",
       "                      -1.1460e-03,  9.0203e-05, -1.8657e-03, -2.3530e-03,  1.5544e-03,\n",
       "                      -1.9832e-03, -1.2505e-04, -2.2830e-03, -1.6789e-03,  3.4263e-03,\n",
       "                      -1.8642e-03, -2.2711e-03, -2.4585e-03, -4.0905e-03, -3.3926e-03,\n",
       "                       7.9787e-04, -1.1952e-03, -6.4047e-04, -3.5775e-04, -4.3295e-05,\n",
       "                       1.3082e-03, -8.7271e-04,  5.0668e-05, -5.0712e-05,  1.5831e-03,\n",
       "                      -2.3415e-03, -2.9289e-05,  4.6378e-04,  1.7682e-03,  1.5483e-03,\n",
       "                      -7.2181e-04, -2.2053e-03, -1.6688e-03, -2.5497e-03, -2.8627e-03,\n",
       "                       1.4017e-03, -2.5845e-03,  1.9897e-04, -4.3420e-04, -3.0279e-03,\n",
       "                       9.2082e-04,  2.3230e-03,  1.5898e-03, -1.4476e-03, -2.3949e-03,\n",
       "                      -1.3684e-03,  2.9032e-03, -1.7322e-03,  1.9843e-03, -8.2979e-04,\n",
       "                      -9.0264e-04])),\n",
       "             ('bn2.running_mean',\n",
       "              tensor([ 0.1653, -0.0387, -0.2354,  0.2383, -0.2065, -0.1302, -0.0650,  0.1439,\n",
       "                      -0.0955,  0.0662, -0.4842, -0.1012, -0.1206, -0.0935,  0.1070,  0.2140,\n",
       "                       0.2251, -0.1398, -0.0394, -0.0229, -0.0055,  0.5666,  0.2786, -0.5269,\n",
       "                      -0.1758,  0.2658,  0.0468,  0.0157, -0.1935, -0.1276, -0.1926,  0.0671,\n",
       "                       0.0845,  0.2996,  0.5706, -0.0639, -0.1425,  0.1101, -0.3858, -0.3458,\n",
       "                       0.1956,  0.1870, -0.1227,  0.0928,  0.2231,  0.0911,  0.1793, -0.1415,\n",
       "                      -0.2471,  0.1701, -0.1580,  0.0706, -0.2724, -0.0876, -0.0223, -0.4392,\n",
       "                      -0.3022, -0.1255, -0.4681, -0.4439, -0.5754,  0.1546, -0.0807, -0.1954,\n",
       "                       0.0058, -0.1138, -0.4677, -0.4566, -0.0369, -0.2254, -0.2334,  0.1171,\n",
       "                      -0.3794,  0.1414, -0.1362, -0.0861, -0.3445, -0.0228,  0.0755, -0.0090,\n",
       "                      -0.3131,  0.1541,  0.0843,  0.0449, -0.1232,  0.1998, -0.2311, -0.2301,\n",
       "                      -0.0655, -0.0829, -0.4354,  0.1872,  0.0734,  0.0918, -0.0297,  0.5336,\n",
       "                      -0.1226, -0.3682,  0.2317, -0.0449,  0.2662,  0.3676,  0.1113, -0.0153,\n",
       "                      -0.2320,  0.1047,  0.4200, -0.0500, -0.2512, -0.0453,  0.3034, -0.2728,\n",
       "                      -0.2825, -0.0455,  0.3081, -0.0550,  0.0665, -0.0495,  0.0317, -0.3228,\n",
       "                      -0.1890,  0.1015, -0.1355, -0.1097,  0.1347, -0.2371, -0.2571, -0.3896,\n",
       "                      -0.0660, -0.1107, -0.0117, -0.0101,  0.1068,  0.0955,  0.2973, -0.2882,\n",
       "                      -0.1865, -0.0359,  0.0084,  0.0192,  0.2682,  0.2698, -0.0364, -0.3646,\n",
       "                       0.2900, -0.1461, -0.2081, -0.1161, -0.2315,  0.1718,  0.0493, -0.0771,\n",
       "                      -0.0592, -0.0553, -0.0472, -0.0013,  0.3030, -0.3267,  0.0631, -0.0177,\n",
       "                       0.2230, -0.0535,  0.3026,  0.0267,  0.3338, -0.3299,  0.2551, -0.3731,\n",
       "                      -0.5004,  0.1163, -0.0517,  0.1256, -0.2723,  0.5282, -0.2679, -0.2731,\n",
       "                      -0.3153,  0.2461,  0.4002, -0.4027,  0.5328,  0.2189, -0.0033,  0.0990,\n",
       "                      -0.2180, -0.1149, -0.4792, -0.2193, -0.3407, -0.6294,  0.2394, -0.1240,\n",
       "                      -0.1815,  0.0171, -0.0653, -0.4143,  0.0274,  0.0306,  0.3369, -0.1703,\n",
       "                      -0.1407, -0.4959, -0.1455, -0.0265,  0.0244, -0.2456, -0.2843,  0.1337,\n",
       "                       0.3381,  0.0339,  0.2913, -0.2672, -0.3103,  0.2350,  0.0290,  0.1735,\n",
       "                       0.0398, -0.4122, -0.3141,  0.0963, -0.0506, -0.0170, -0.0393, -0.0130,\n",
       "                       0.0505,  0.0197,  0.1424, -0.2213, -0.0351, -0.3419,  0.0097, -0.3109,\n",
       "                       0.2731, -0.1591,  0.2382, -0.3295,  0.0808,  0.0313,  0.3034,  0.0874,\n",
       "                      -0.0990,  0.0856,  0.0194, -0.3529,  0.4212, -0.5982,  0.0681,  0.1163,\n",
       "                       0.0315, -0.4979, -0.0122,  0.1321, -0.4076, -0.0857, -0.1814, -0.1860])),\n",
       "             ('bn2.running_var',\n",
       "              tensor([0.2073, 0.2027, 0.2016, 0.1729, 0.2126, 0.2003, 0.2233, 0.2252, 0.2483,\n",
       "                      0.1724, 0.1999, 0.2834, 0.2414, 0.2360, 0.1977, 0.1893, 0.2081, 0.2180,\n",
       "                      0.1530, 0.2564, 0.2138, 0.1615, 0.2876, 0.1996, 0.1659, 0.1625, 0.1928,\n",
       "                      0.1823, 0.1623, 0.2215, 0.1709, 0.2274, 0.2263, 0.1956, 0.3111, 0.1753,\n",
       "                      0.2687, 0.1473, 0.1936, 0.1788, 0.2001, 0.1521, 0.2073, 0.1730, 0.2043,\n",
       "                      0.2176, 0.1883, 0.1794, 0.2067, 0.2820, 0.2192, 0.2241, 0.1713, 0.2425,\n",
       "                      0.1716, 0.2080, 0.1877, 0.2924, 0.2129, 0.2098, 0.2081, 0.1902, 0.1756,\n",
       "                      0.2127, 0.1589, 0.1630, 0.2450, 0.1503, 0.1844, 0.1747, 0.1725, 0.1665,\n",
       "                      0.2649, 0.1939, 0.1731, 0.2953, 0.1743, 0.1992, 0.2321, 0.2338, 0.2101,\n",
       "                      0.2079, 0.3056, 0.1784, 0.2081, 0.2249, 0.2346, 0.2206, 0.1665, 0.2090,\n",
       "                      0.1769, 0.1856, 0.1794, 0.1638, 0.1593, 0.1978, 0.1992, 0.1499, 0.1980,\n",
       "                      0.2569, 0.1796, 0.2675, 0.1960, 0.1803, 0.2677, 0.1703, 0.2045, 0.1923,\n",
       "                      0.1925, 0.3004, 0.2615, 0.1703, 0.1940, 0.1727, 0.2918, 0.1552, 0.1804,\n",
       "                      0.1900, 0.1740, 0.2213, 0.1678, 0.1685, 0.2028, 0.1689, 0.2123, 0.2007,\n",
       "                      0.1738, 0.2268, 0.2601, 0.1608, 0.2085, 0.1700, 0.1905, 0.2297, 0.1890,\n",
       "                      0.2260, 0.1757, 0.1946, 0.1789, 0.1737, 0.2624, 0.2073, 0.1703, 0.1984,\n",
       "                      0.1666, 0.1731, 0.1631, 0.1944, 0.1704, 0.2187, 0.2692, 0.1650, 0.1923,\n",
       "                      0.2052, 0.1669, 0.1492, 0.2743, 0.2081, 0.1721, 0.2196, 0.2959, 0.1869,\n",
       "                      0.2034, 0.1741, 0.2640, 0.2251, 0.2293, 0.1523, 0.2236, 0.2941, 0.1702,\n",
       "                      0.2412, 0.2567, 0.2619, 0.2183, 0.2365, 0.2108, 0.2134, 0.2109, 0.2006,\n",
       "                      0.2234, 0.1945, 0.1529, 0.1916, 0.2027, 0.3445, 0.2328, 0.2447, 0.1817,\n",
       "                      0.2317, 0.1505, 0.1827, 0.2124, 0.1691, 0.1700, 0.1698, 0.2053, 0.3340,\n",
       "                      0.2595, 0.2015, 0.2041, 0.2264, 0.2181, 0.2112, 0.2024, 0.1891, 0.2190,\n",
       "                      0.1785, 0.2538, 0.3131, 0.2123, 0.1657, 0.2431, 0.1921, 0.2039, 0.2005,\n",
       "                      0.1937, 0.1673, 0.2162, 0.1732, 0.2023, 0.1673, 0.2572, 0.1718, 0.2177,\n",
       "                      0.1962, 0.3299, 0.1788, 0.2030, 0.1570, 0.2049, 0.2106, 0.2359, 0.2137,\n",
       "                      0.2229, 0.2343, 0.1720, 0.1892, 0.1871, 0.1649, 0.2181, 0.1951, 0.2021,\n",
       "                      0.1535, 0.4046, 0.1803, 0.1862, 0.2378, 0.1667, 0.2036, 0.2993, 0.2079,\n",
       "                      0.1637, 0.2189, 0.1805, 0.1961])),\n",
       "             ('bn2.num_batches_tracked', tensor(3150)),\n",
       "             ('fc3.weight',\n",
       "              tensor([[-0.0680,  0.0235, -0.0253,  ..., -0.0481, -0.0223,  0.0176],\n",
       "                      [ 0.0240,  0.0403,  0.0591,  ...,  0.0145,  0.0073,  0.0405],\n",
       "                      [ 0.0512, -0.0343, -0.0015,  ..., -0.0577, -0.0567, -0.0313],\n",
       "                      ...,\n",
       "                      [ 0.0169,  0.0411, -0.0109,  ..., -0.0443, -0.0535,  0.0355],\n",
       "                      [ 0.0061, -0.0595,  0.0468,  ...,  0.0263,  0.0516,  0.0550],\n",
       "                      [ 0.0266, -0.0179, -0.0619,  ...,  0.0134, -0.0203,  0.0217]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([ 0.0244, -0.0300,  0.0068,  0.0180,  0.0254,  0.0409, -0.0394, -0.0493,\n",
       "                      -0.0161,  0.0240,  0.0463, -0.0582,  0.0253,  0.0008,  0.0274,  0.0242,\n",
       "                       0.0273,  0.0163, -0.0153,  0.0323, -0.0060,  0.0435,  0.0205, -0.0222,\n",
       "                      -0.0219,  0.0212, -0.0192,  0.0487, -0.0319, -0.0063,  0.0444,  0.0259,\n",
       "                      -0.0114, -0.0455, -0.0069, -0.0218,  0.0594, -0.0065, -0.0103, -0.0015,\n",
       "                       0.0090,  0.0278, -0.0484, -0.0286,  0.0333, -0.0422,  0.0316,  0.0360,\n",
       "                       0.0215,  0.0321,  0.0140, -0.0360,  0.0309, -0.0587,  0.0357, -0.0049,\n",
       "                      -0.0259,  0.0164, -0.0493,  0.0137, -0.0234,  0.0405,  0.0492, -0.0308])),\n",
       "             ('bn3.weight',\n",
       "              tensor([0.9637, 1.0648, 1.0587, 1.0614, 1.0613, 1.0513, 1.0649, 1.0627, 1.0592,\n",
       "                      0.9722, 1.0651, 1.0545, 1.0529, 1.0284, 1.0512, 1.0556, 0.9710, 0.9708,\n",
       "                      1.0515, 0.9706, 0.9699, 1.0030, 1.0565, 0.9693, 0.9881, 1.0382, 0.9849,\n",
       "                      0.9712, 1.0507, 0.9670, 0.9724, 1.0579, 0.9725, 1.0532, 1.0539, 1.0569,\n",
       "                      1.0520, 1.0512, 1.0526, 0.9717, 0.9705, 1.0574, 0.9730, 0.9661, 1.0513,\n",
       "                      0.9698, 0.9724, 1.0529, 1.0692, 1.0659, 0.9703, 0.9732, 1.0574, 0.9676,\n",
       "                      1.0518, 1.0596, 1.0489, 0.9706, 1.0209, 1.0540, 1.0550, 1.0140, 0.9718,\n",
       "                      1.0638])),\n",
       "             ('bn3.bias',\n",
       "              tensor([-0.0418,  0.0677,  0.0612,  0.0657,  0.0649,  0.0536,  0.0695,  0.0665,\n",
       "                       0.0612, -0.0362,  0.0689,  0.0567,  0.0541,  0.0312,  0.0534,  0.0594,\n",
       "                      -0.0355, -0.0375,  0.0519, -0.0361, -0.0363,  0.0039,  0.0587, -0.0357,\n",
       "                      -0.0122,  0.0409, -0.0161, -0.0323,  0.0524, -0.0359, -0.0352,  0.0601,\n",
       "                      -0.0349,  0.0563,  0.0564,  0.0598,  0.0529,  0.0527,  0.0545, -0.0355,\n",
       "                      -0.0342,  0.0599, -0.0316, -0.0377,  0.0536, -0.0357, -0.0325,  0.0546,\n",
       "                       0.0716,  0.0699, -0.0364, -0.0288,  0.0598, -0.0367,  0.0522,  0.0625,\n",
       "                       0.0506, -0.0351,  0.0225,  0.0566,  0.0580,  0.0152, -0.0369,  0.0666])),\n",
       "             ('bn3.running_mean',\n",
       "              tensor([-0.5117, -0.1184, -0.3289, -0.4470, -0.2408, -0.1401,  0.1464, -0.0215,\n",
       "                       0.0746,  0.0231, -0.3257, -0.1929,  0.0354, -0.1766, -0.0819, -0.1509,\n",
       "                      -0.1259, -0.0871,  0.0683,  0.2328,  0.1626, -0.3387, -0.2139,  0.2816,\n",
       "                      -0.4981, -0.1378, -0.3920,  0.2723, -0.1327,  0.0709,  0.0189,  0.2833,\n",
       "                       0.0278, -0.2507, -0.3066,  0.1441, -0.2177, -0.4912, -0.5018,  0.3446,\n",
       "                       0.4263, -0.0749,  0.5103, -0.5981, -0.3548,  0.3213,  0.2084, -0.1874,\n",
       "                      -0.0679,  0.2616,  0.1808, -0.2330,  0.0271,  0.0245, -0.0180, -0.5013,\n",
       "                      -0.3692,  0.2344,  0.0828, -0.2442,  0.2553,  0.2717,  0.1196,  0.0937])),\n",
       "             ('bn3.running_var',\n",
       "              tensor([0.3521, 0.1427, 0.2002, 0.1696, 0.1623, 0.2965, 0.1660, 0.2026, 0.2158,\n",
       "                      0.6479, 0.1605, 0.1771, 0.2359, 0.1935, 0.3203, 0.1611, 0.5107, 0.6474,\n",
       "                      0.2814, 0.4228, 0.3786, 0.1630, 0.1941, 0.4007, 0.2834, 0.1607, 0.2614,\n",
       "                      0.3719, 0.2080, 0.2621, 0.5810, 0.1828, 0.5554, 0.2029, 0.2064, 0.2048,\n",
       "                      0.2113, 0.2151, 0.1772, 0.5510, 0.5723, 0.1838, 0.3682, 0.2512, 0.1789,\n",
       "                      0.3974, 0.3642, 0.1863, 0.1694, 0.2103, 0.4484, 0.1653, 0.1592, 0.4258,\n",
       "                      0.3176, 0.2249, 0.1483, 0.3972, 0.1864, 0.1559, 0.2435, 0.1559, 0.5984,\n",
       "                      0.1881])),\n",
       "             ('bn3.num_batches_tracked', tensor(3150)),\n",
       "             ('fc4.weight',\n",
       "              tensor([[ 0.0300, -0.0543, -0.0693, -0.0414, -0.0577, -0.1570, -0.0426, -0.0541,\n",
       "                       -0.0661,  0.0722, -0.0432, -0.0920, -0.1193, -0.0296, -0.1509, -0.0680,\n",
       "                        0.0420,  0.0914, -0.1580,  0.0710,  0.0450, -0.0208, -0.0762,  0.0631,\n",
       "                       -0.0141, -0.0334, -0.0126,  0.0192, -0.1620,  0.0344,  0.0558, -0.0686,\n",
       "                        0.0458, -0.0964, -0.0900, -0.0726, -0.1346, -0.1476, -0.1087,  0.0752,\n",
       "                        0.0507, -0.0722,  0.0174,  0.0172, -0.1388,  0.0538,  0.0230, -0.1091,\n",
       "                       -0.0451, -0.0517,  0.0910,  0.0010, -0.0726,  0.0556, -0.1461, -0.0640,\n",
       "                       -0.0378,  0.0407, -0.0275, -0.0880, -0.0795, -0.0251,  0.0855, -0.0430]])),\n",
       "             ('fc4.bias', tensor([0.0582]))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_Model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "vocab_size=50000\n",
    "embedding_dim=128\n",
    "max_length=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_model = LSTMModel(vocal_size=vocab_size, embedding_dim=embedding_dim,hidden_size=32, length=max_length, output_dim=1)\n",
    "predict_model.load_state_dict(torch.load('best_model_lstm_vihsd.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dự đoán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocess test with 3 type: ViCTSD, ViHSD, ViMergre\n",
    "def test_dataloader(dataset_test_type):\n",
    "    test = load_data(set_name='test', dataset=dataset_test_type)\n",
    "    test['label'] = test['label'].replace(2,1)\n",
    "    test['text'] = test['text'].apply(tokenize)\n",
    "    test_preprocess = preprocess_data(test,\n",
    "                                    url=True,\n",
    "                                    punctuation=True,\n",
    "                                    lowercase=True,\n",
    "                                    stopword=True,\n",
    "                                    special_stopwords=special_stopwords,\n",
    "                                    emoji=True)\n",
    "    X_test = test_preprocess['text'].astype(str)\n",
    "    y_test = test_preprocess['label']\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test = pad_sequences(X_test, maxlen=max_length, padding='post',truncating='post')\n",
    "    test_data = CustomDataset(X_test, y_test)\n",
    "    test_dataloader = DataLoader(dataset=test_data,\n",
    "                                    batch_sampler=StratifiedBatchSampler(torch.tensor(test_data.y_encoded), batch_size=BATCH_SIZE))\n",
    "    return test_dataloader\n",
    "\n",
    "vihsd_test = test_dataloader('vihsd')\n",
    "victsd_test = test_dataloader('victsd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0188,   Test acc: 0.7065,   F1 test score: 0.1489, F1 test macro: 0.4851\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = 0,0\n",
    "f1_test_pos_1 = []\n",
    "f1_macro = []\n",
    "predict_model.eval()\n",
    "y_pred_list = []\n",
    "y_real_list = []\n",
    "loss_fn = nn.BCELoss()\n",
    "for batch, (X_test_, y_test_) in enumerate(victsd_test):\n",
    "    # 1. Forward pass\n",
    "    X_test_ = X_test_.long()\n",
    "    test_pred = predict_model(X_test_)\n",
    "\n",
    "    y_pred_list.append(test_pred.tolist())\n",
    "    y_real_list.append(y_test_.tolist())\n",
    "\n",
    "    # 2. Calculate the loss (accumulatively)\n",
    "    test_loss += loss_fn(test_pred.squeeze(dim=1), y_test_.float()).item()\n",
    "    # print(test_pred.shape)\n",
    "    # 3. Calculate accuracy\n",
    "    acc = accuracy_fn(y_true= y_test_.float(),\n",
    "                    y_pred = test_pred.squeeze(dim=1))\n",
    "    acc = (torch.round(test_pred)==y_test_).sum().item()/len(y_test_)\n",
    "    test_acc += acc\n",
    "    # 4. Calculate f1 score\n",
    "    pos_1, macro = f1_score_fn(y_true= y_test_.float(),\n",
    "                            y_pred = test_pred.squeeze(dim=1))\n",
    "    f1_test_pos_1.append(pos_1.item())\n",
    "    f1_macro.append(macro.item())\n",
    "# Calculate the test loss average per batch\n",
    "test_loss /= len(victsd_test)\n",
    "# test_loss /= len(test_dataloader)\n",
    "f1_test_pos_1 = sum(f1_test_pos_1)/len(f1_test_pos_1)\n",
    "f1_test_macro = sum(f1_macro)/len(f1_macro)\n",
    "# Calculate the test acc average per batch\n",
    "test_acc /= len(victsd_test)\n",
    "print(f\"Test loss: {test_loss:.4f},   Test acc: {test_acc:.4f},   F1 test score: {f1_test_pos_1:.4f}, F1 test macro: {f1_test_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViCTSD: Test loss: 0.0188,   Test acc: 0.7065,   F1 test score: 0.1548, F1 test macro: 0.4883\n",
    "ViHSD: Test loss: 0.0131,   Test acc: 0.7261,   F1 test score: 0.4678, F1 test macro: 0.6828\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_y_pred_list = sum(y_pred_list, [])\n",
    "flat_y_pred_list = sum(flat_y_pred_list, [])\n",
    "flat_y_pred_list = [round(x) for x in flat_y_pred_list]\n",
    "flat_y_real_list = sum(y_real_list,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.77      0.82       890\n",
      "           1       0.11      0.25      0.16       110\n",
      "\n",
      "    accuracy                           0.71      1000\n",
      "   macro avg       0.50      0.51      0.49      1000\n",
      "weighted avg       0.81      0.71      0.75      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true=flat_y_real_list, y_pred=flat_y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(tokenizer.vocab_size, 64))\n",
    "model1.add(SimpleRNN(32))\n",
    "model1.add(Dense(16, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
