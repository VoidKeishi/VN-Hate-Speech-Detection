{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huấn luyện trên bộ dữ liệu ViHSD\n",
    "* Bao gồm dữ liệu thu thập từ mạng xã hội\n",
    "* Dữ liệu có tính toxic cao - phân biệt chủng tộc, vùng miền, công kích cá nhân, chửi đổng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvi\n",
    "from utility.utility import load_data\n",
    "import string\n",
    "import emoji_vietnamese  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup DATA for train and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_type = 'victsd'\n",
    "train = load_data(set_name='train', dataset=dataset_train_type)\n",
    "dev = load_data(set_name='dev', dataset=dataset_train_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup DATA for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_type = 'victsd'\n",
    "test = load_data(set_name='test', dataset=dataset_test_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'], dev['label'], test['label'] = train['label'].replace(2,1), dev['label'].replace(2,1), test['label'].replace(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "* Remove url in comment\n",
    "* remove punctuation\n",
    "* Lowercase data\n",
    "* Remove stopwords\n",
    "* Remove emoji\n",
    "* Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Thật tuyệt vời -> Thật tuyệt_vời\n",
    "    \"\"\"\n",
    "    return ViTokenizer.tokenize(text)\n",
    "\n",
    "# apply tokenize to text\n",
    "train['text'] = train['text'].apply(tokenize)\n",
    "dev['text'] = dev['text'].apply(tokenize)\n",
    "test['text'] = test['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    data,\n",
    "    url=True,\n",
    "    punctuation=True,\n",
    "    lowercase=True,\n",
    "    stopword=False,\n",
    "    special_stopwords=[],\n",
    "    emoji=False\n",
    "):\n",
    "    # Load stopwords\n",
    "    with open('./utility/Stopwords/vietnamese-stopwords-dash.txt', 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "    for word in special_stopwords:\n",
    "        stopwords.remove(word)\n",
    "    # Function to remove stopwords\n",
    "    def remove_stopwords(text):\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "        return ' '.join(words)\n",
    "    if url:\n",
    "        # Remove URLs\n",
    "        data['text'] = data['text'].str.replace(\n",
    "            r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "    if punctuation:\n",
    "        # Remove punctuation\n",
    "        data['text'] = data['text'].str.replace(\n",
    "            '['+string.punctuation+']', '', regex=True)\n",
    "    if lowercase:\n",
    "        # Lowercase\n",
    "        data['text'] = data['text'].str.lower()\n",
    "    if stopword:\n",
    "        # Remove stopword\n",
    "        data['text'] = data['text'].apply(remove_stopwords)\n",
    "    if emoji:\n",
    "        # Remove emojis\n",
    "        data['text'] = data['text'].apply(emoji_vietnamese.demojize)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_stopwords = [\"không\",\"không_có\",\"không_thể\",\"chưa\", \"được\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocess = preprocess_data(train,\n",
    "                                   url=True,\n",
    "                                   punctuation=True,\n",
    "                                   lowercase=True,\n",
    "                                   stopword=True,\n",
    "                                   special_stopwords=special_stopwords,\n",
    "                                   emoji=False)\n",
    "dev_preprocess = preprocess_data(dev,\n",
    "                                 url=True,\n",
    "                                 punctuation=True,\n",
    "                                 lowercase=True,\n",
    "                                 stopword=True,\n",
    "                                 special_stopwords=special_stopwords,\n",
    "                                 emoji=False)\n",
    "test_preprocess = preprocess_data(test,\n",
    "                                  url=True,\n",
    "                                  punctuation=True,\n",
    "                                  lowercase=True,\n",
    "                                  stopword=True,\n",
    "                                  special_stopwords=special_stopwords,\n",
    "                                  emoji=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['text'].astype(str)\n",
    "y_train = train['label']\n",
    "X_dev = dev['text'].astype(str)\n",
    "y_dev = dev['label']\n",
    "X_test = test['text'].astype(str)\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Để lấy dữ liệu cho nhanh chứ ko up lên git\n",
    "train_preprocess.to_csv('ctsd_preprocessed_train_data.csv', index=False)\n",
    "dev_preprocess.to_csv('ctsd_preprocessed_dev_data.csv', index=False)\n",
    "test_preprocess.to_csv('ctsd_preprocessed_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from csv\n",
    "* Tokenizer and pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "vocab_size=50000\n",
    "embedding_dim=128\n",
    "max_length=128\n",
    "\n",
    "train_data = pd.read_csv('ctsd_preprocessed_train_data.csv')\n",
    "dev_data = pd.read_csv('ctsd_preprocessed_dev_data.csv')\n",
    "test_data = pd.read_csv('ctsd_preprocessed_test_data.csv')\n",
    "\n",
    "# ## Lấy 1 phần dữ liệu để chạy nhanh\n",
    "# train_data = train_data[:2254]\n",
    "# dev_data = dev_data[:267]\n",
    "\n",
    "# X_train, y_train,_,_ = train_test_split(train_data['text'], train_data['label'], test_size=0.99, random_state=42)\n",
    "# X_val, y_val,_,_ = train_test_split(dev_data['text'], dev_data['label'], test_size=0.99, random_state=42)\n",
    "# X_train= X_train.astype(str)\n",
    "# X_val= X_val.astype(str)\n",
    "\n",
    "X_train, y_train = train_data['text'].astype(str), train_data['label']\n",
    "X_dev, y_dev = dev_data['text'].astype(str), dev_data['label']\n",
    "X_test, y_test = test_data['text'].astype(str), test_data['label']\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='post',truncating='post')\n",
    "\n",
    "# Thực hiện thay đổi test để đưa vào tính toán val_acc\n",
    "X_dev = tokenizer.texts_to_sequences(X_dev)\n",
    "X_dev = pad_sequences(X_dev, maxlen=max_length, padding='post',truncating='post')\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "  y_pred_rounded = torch.round(y_pred)  \n",
    "  correct = torch.eq(y_true, y_pred_rounded).sum().item()\n",
    "  acc = (correct/len(y_pred))*100\n",
    "  return acc\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "def f1_score_fn(y_true, y_pred):\n",
    "  y_true = y_true.int().tolist()\n",
    "  y_pred = torch.round(y_pred).int().tolist()\n",
    "  f1_score_pos1 = f1_score(y_true=y_true, y_pred=y_pred, pos_label=1)\n",
    "  f1_score_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "  # print(classification_report(y_true=y_true, y_pred=y_pred,zero_division=1))  \n",
    "  return f1_score_pos1, f1_score_macro_average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_encoded: np.ndarray, y_encoded: pd.core.series.Series):\n",
    "        # Setup\n",
    "        self.x_encoded = x_encoded\n",
    "        self.y_encoded = y_encoded.tolist()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.FloatTensor(self.x_encoded[idx]), self.y_encoded[idx])\n",
    "        # return (self.x_encoded[idx], self.y_encoded[idx])\n",
    "        # return {'text': self.x[idx], 'label': self.y_encoded[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x_encoded.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(X_train, y_train)\n",
    "dev_data = CustomDataset(X_dev, y_dev)\n",
    "test_data = CustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class StratifiedBatchSampler:\n",
    "    \"\"\"Stratified batch sampling\n",
    "    Provides equal representation of target classes in each batch\n",
    "    \"\"\"\n",
    "    def __init__(self, y, batch_size, shuffle=True):\n",
    "        if torch.is_tensor(y):\n",
    "            y = y.numpy()\n",
    "        assert len(y.shape) == 1, 'label array must be 1D'\n",
    "        n_batches = int(len(y) / batch_size)\n",
    "        self.skf = StratifiedKFold(n_splits=n_batches, shuffle=shuffle)\n",
    "        self.X = torch.randn(len(y),1).numpy()\n",
    "        self.y = y\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.skf.random_state = torch.randint(0,int(1e8),size=()).item()\n",
    "        for train_idx, test_idx in self.skf.split(self.X, self.y):\n",
    "            yield test_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng worker tối đa: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "print(f\"Số lượng worker tối đa: {num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE=32\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_sampler=StratifiedBatchSampler(torch.tensor(train_data.y_encoded), batch_size=BATCH_SIZE))\n",
    "dev_dataloader = DataLoader(dataset=dev_data,\n",
    "                              batch_sampler=StratifiedBatchSampler(torch.tensor(dev_data.y_encoded), batch_size=BATCH_SIZE))                            \n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                                batch_sampler=StratifiedBatchSampler(torch.tensor(test_data.y_encoded), batch_size=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocal_size, embedding_dim, hidden_size, output_dim, length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=vocal_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)\n",
    "        self.dropout_lstm = nn.Dropout(p=0.2)  # Dropout sau LSTM\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=length*hidden_size, out_features=length*hidden_size//4)\n",
    "        self.bn1 = nn.BatchNorm1d(length*hidden_size//4) \n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=length*hidden_size//4, out_features=length*hidden_size//16)\n",
    "        self.bn2 = nn.BatchNorm1d(length*hidden_size//16) \n",
    "\n",
    "        self.fc3 = nn.Linear(in_features=length*hidden_size//16, out_features=length*hidden_size//64)\n",
    "        self.bn3 = nn.BatchNorm1d(length*hidden_size//64) \n",
    "\n",
    "        self.fc4 = nn.Linear(in_features=length*hidden_size//64, out_features=output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout_lstm(x) # Áp dụng Dropout sau LSTM\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x) \n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embed): Embedding(50000, 128)\n",
       "  (lstm): LSTM(128, 32, batch_first=True)\n",
       "  (dropout_lstm): Dropout(p=0.2, inplace=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_Model = LSTMModel(vocal_size=vocab_size, embedding_dim=embedding_dim,hidden_size=32, length=max_length, output_dim=1)\n",
    "LSTM_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 6241, 1: 759})\n"
     ]
    }
   ],
   "source": [
    "def weighted_binary_cross_entropy(y_true, y_pred, pos_weight):\n",
    "    \"\"\"\n",
    "    Weighted Binary Cross Entropy (WBCE) = - (w * y * log(p) + (1 - y) * log(1 - p))\n",
    "    Trong đó:\n",
    "    y: Nhãn thực tế (0 hoặc 1).\n",
    "    p: Xác suất dự đoán cho lớp positive (nhãn 1).\n",
    "    w: Trọng số cho lớp positive.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-7\n",
    "    y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)  # giới hạn giá trị dự đoán trong khoảng (epsilon, 1 - epsilon)\n",
    "    bce = - (pos_weight * y_true * torch.log(y_pred) +\n",
    "              (1 - y_true) * torch.log(1 - y_pred))     # Binary Cross Entropy trong đo tăng tầm quan trọng khi dự đoán sai lớp 1\n",
    "    return torch.mean(bce)\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(y_train))\n",
    "neg_count = Counter(y_train)[0]\n",
    "pos_count = Counter(y_train)[1]\n",
    "pos_weight = torch.tensor((neg_count/2) / pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(LSTM_Model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae289fbf8d2d4401a293b36c3c91aff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "------\n",
      "\n",
      "Train loss: 0.0309, Train acc: 0.3080, F1 train pos 1 score: 0.1835, F1 train macro score: 0.2865 | \n",
      "Test loss: 0.0300,   Test acc: 0.2473,   F1 test pos 1 score: 0.2110, F1 test macro scoreL 0.2492\n",
      "save model at this epoch\n",
      "Epoch: 1\n",
      "------\n",
      "\n",
      "Train loss: 0.0295, Train acc: 0.4142, F1 train pos 1 score: 0.1946, F1 train macro score: 0.3700 | \n",
      "Test loss: 0.0290,   Test acc: 0.6619,   F1 test pos 1 score: 0.2005, F1 test macro scoreL 0.4981\n",
      "save model at this epoch\n",
      "Epoch: 2\n",
      "------\n",
      "\n",
      "Train loss: 0.0287, Train acc: 0.5243, F1 train pos 1 score: 0.2018, F1 train macro score: 0.4379 | \n",
      "Test loss: 0.0291,   Test acc: 0.5693,   F1 test pos 1 score: 0.2310, F1 test macro scoreL 0.4758\n",
      "Epoch: 3\n",
      "------\n",
      "\n",
      "Train loss: 0.0280, Train acc: 0.6243, F1 train pos 1 score: 0.2059, F1 train macro score: 0.4873 | \n",
      "Test loss: 0.0287,   Test acc: 0.6725,   F1 test pos 1 score: 0.2181, F1 test macro scoreL 0.5128\n",
      "save model at this epoch\n",
      "Epoch: 4\n",
      "------\n",
      "\n",
      "Train loss: 0.0272, Train acc: 0.6910, F1 train pos 1 score: 0.2147, F1 train macro score: 0.5195 | \n",
      "Test loss: 0.0283,   Test acc: 0.7731,   F1 test pos 1 score: 0.1885, F1 test macro scoreL 0.5335\n",
      "save model at this epoch\n",
      "Epoch: 5\n",
      "------\n",
      "\n",
      "Train loss: 0.0269, Train acc: 0.7438, F1 train pos 1 score: 0.2017, F1 train macro score: 0.5308 | \n",
      "Test loss: 0.0281,   Test acc: 0.7842,   F1 test pos 1 score: 0.1797, F1 test macro scoreL 0.5319\n",
      "save model at this epoch\n",
      "Epoch: 6\n",
      "------\n",
      "\n",
      "Train loss: 0.0262, Train acc: 0.7763, F1 train pos 1 score: 0.2374, F1 train macro score: 0.5629 | \n",
      "Test loss: 0.0281,   Test acc: 0.7715,   F1 test pos 1 score: 0.1730, F1 test macro scoreL 0.5238\n",
      "Epoch: 7\n",
      "------\n",
      "\n",
      "Train loss: 0.0258, Train acc: 0.7979, F1 train pos 1 score: 0.2417, F1 train macro score: 0.5722 | \n",
      "Test loss: 0.0281,   Test acc: 0.7610,   F1 test pos 1 score: 0.1647, F1 test macro scoreL 0.5157\n",
      "Epoch: 8\n",
      "------\n",
      "\n",
      "Train loss: 0.0251, Train acc: 0.8028, F1 train pos 1 score: 0.2777, F1 train macro score: 0.5941 | \n",
      "Test loss: 0.0280,   Test acc: 0.7588,   F1 test pos 1 score: 0.1948, F1 test macro scoreL 0.5324\n",
      "Epoch: 9\n",
      "------\n",
      "\n",
      "Train loss: 0.0245, Train acc: 0.8005, F1 train pos 1 score: 0.3012, F1 train macro score: 0.6063 | \n",
      "Test loss: 0.0280,   Test acc: 0.7442,   F1 test pos 1 score: 0.1978, F1 test macro scoreL 0.5284\n",
      "Epoch: 10\n",
      "------\n",
      "\n",
      "Train loss: 0.0237, Train acc: 0.7858, F1 train pos 1 score: 0.3452, F1 train macro score: 0.6272 | \n",
      "Test loss: 0.0278,   Test acc: 0.7526,   F1 test pos 1 score: 0.1882, F1 test macro scoreL 0.5261\n",
      "Epoch: 11\n",
      "------\n",
      "\n",
      "Train loss: 0.0231, Train acc: 0.7817, F1 train pos 1 score: 0.3563, F1 train macro score: 0.6324 | \n",
      "Test loss: 0.0281,   Test acc: 0.7113,   F1 test pos 1 score: 0.2024, F1 test macro scoreL 0.5190\n",
      "Epoch: 12\n",
      "------\n",
      "\n",
      "Train loss: 0.0225, Train acc: 0.7729, F1 train pos 1 score: 0.3794, F1 train macro score: 0.6430 | \n",
      "Test loss: 0.0278,   Test acc: 0.7432,   F1 test pos 1 score: 0.2120, F1 test macro scoreL 0.5364\n",
      "Epoch: 13\n",
      "------\n",
      "\n",
      "Train loss: 0.0218, Train acc: 0.7621, F1 train pos 1 score: 0.3996, F1 train macro score: 0.6517 | \n",
      "Test loss: 0.0281,   Test acc: 0.7207,   F1 test pos 1 score: 0.2144, F1 test macro scoreL 0.5299\n",
      "Epoch: 14\n",
      "------\n",
      "\n",
      "Train loss: 0.0209, Train acc: 0.7610, F1 train pos 1 score: 0.4160, F1 train macro score: 0.6612 | \n",
      "Test loss: 0.0283,   Test acc: 0.6896,   F1 test pos 1 score: 0.2307, F1 test macro scoreL 0.5277\n"
     ]
    }
   ],
   "source": [
    "# Write a training and evaluationg loop for model_1\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Train for longer\n",
    "epochs = 15\n",
    "\n",
    "# # Put data on the target device\n",
    "# X_padded_sequences, y_train = torch.tensor(X_padded_sequences).to(device), torch.tensor(y_train).to(device)\n",
    "# padded_val_sequences, y_test=  torch.tensor(padded_val_sequences).to(device), torch.tensor(y_test).to(device)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "best_f1_score = 0\n",
    "best_acc_score = 0\n",
    "\n",
    "# Create training and test loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  print(f\"Epoch: {epoch}\\n------\")\n",
    "  ### Training\n",
    "  train_loss, train_acc=0,0\n",
    "  f1_train_pos_1 = []\n",
    "  f1_train_macro = []\n",
    "  cnt = 0\n",
    "  f1_score_list = []\n",
    "  LSTM_Model.train()\n",
    "  # Add a loop to loop through the training batches\n",
    "  for batch, (X, y) in enumerate(train_dataloader):\n",
    "    cnt+=1\n",
    "    # 1. Forward\n",
    "    X = X.long()\n",
    "    y_pred = LSTM_Model(X)\n",
    "\n",
    "    # 2. Calculate the loss\n",
    "    # loss = loss_fn(y_pred.squeeze(), y.float().squeeze())\n",
    "    loss = weighted_binary_cross_entropy(y_true=y.float().squeeze(), y_pred=y_pred.squeeze(), pos_weight=pos_weight)\n",
    "    train_loss += loss.item()\n",
    "    f1_pos_1, f1_macro = f1_score_fn(y_true= y.float(),\n",
    "                            y_pred = y_pred.squeeze(dim=1))\n",
    "    f1_train_pos_1.append(f1_pos_1.item())\n",
    "    f1_train_macro.append(f1_macro.item())\n",
    "\n",
    "    # 3.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4.\n",
    "    loss.backward()\n",
    "\n",
    "    # 5.\n",
    "    optimizer.step()\n",
    "\n",
    "    # 6. Calculate accuracy metric\n",
    "    y_pred_class = torch.round(y_pred)\n",
    "    train_acc += (y_pred_class==y).sum().item()/len(y_pred_class)\n",
    "\n",
    "  # Divide total train loss by length of train dataloader\n",
    "  # train_loss\n",
    "  train_loss /= len(train_dataloader)\n",
    "  train_acc /= len(train_dataloader)\n",
    "  f1_train_pos_1 = sum(f1_train_pos_1)/len(f1_train_pos_1)\n",
    "  f1_train_macro = sum(f1_train_macro)/len(f1_train_macro)\n",
    "\n",
    "  ### Testing\n",
    "  test_loss, test_acc = 0,0\n",
    "  f1_test_pos_1 = []\n",
    "  f1_test_macro = []\n",
    "  LSTM_Model.eval()\n",
    "  with torch.inference_mode():\n",
    "    for batch, (X_test_, y_test_) in enumerate(dev_dataloader):\n",
    "      # 1. Forward pass\n",
    "      X_test_ = X_test_.long()\n",
    "      test_pred = LSTM_Model(X_test_)\n",
    "\n",
    "      # 2. Calculate the loss (accumulatively)\n",
    "      test_loss += weighted_binary_cross_entropy(\n",
    "                y_test_.float(), test_pred.squeeze(dim=1), pos_weight)\n",
    "      # print(test_pred.shape)\n",
    "      # 3. Calculate accuracy\n",
    "      acc = accuracy_fn(y_true= y_test_.float(),\n",
    "                        y_pred = test_pred.squeeze(dim=1))\n",
    "      acc = (torch.round(test_pred)==y_test_).sum().item()/len(y_test_)\n",
    "      test_acc += acc\n",
    "      # 4. Calculate f1 score\n",
    "      pos_1, macro = f1_score_fn(y_true= y_test_.float(),\n",
    "                              y_pred = test_pred.squeeze(dim=1))\n",
    "      f1_test_pos_1.append(pos_1.item())\n",
    "      f1_test_macro.append(macro.item())\n",
    "\n",
    "    # Calculate the test loss average per batch\n",
    "    test_loss /= len(dev_dataloader)\n",
    "    # test_loss /= len(test_dataloader)\n",
    "    f1_test_pos_1 = sum(f1_test_pos_1)/len(f1_test_pos_1)\n",
    "    f1_test_macro = sum(f1_test_macro)/len(f1_test_macro)\n",
    "    # Calculate the test acc average per batch\n",
    "    test_acc /= len(dev_dataloader)\n",
    "\n",
    "  # print out what happen\n",
    "  print(f\"\\nTrain loss: {train_loss:.4f}, Train acc: {train_acc:.4f}, F1 train pos 1 score: {f1_train_pos_1:.4f}, F1 train macro score: {f1_train_macro:.4f} | \\nTest loss: {test_loss:.4f},   Test acc: {test_acc:.4f},   F1 test pos 1 score: {f1_test_pos_1:.4f}, F1 test macro scoreL {f1_test_macro:.4f}\")\n",
    "  # print(f\"\\nTrain loss; {train_loss:.4f}\")\n",
    "\n",
    "  # Lưu trữ trọng số mô hình tốt nhất\n",
    "  if f1_test_pos_1 > best_f1_score-0.03 and test_acc > best_acc_score-0.03 and f1_test_pos_1 + test_acc > best_f1_score+best_acc_score:\n",
    "    best_f1_score = f1_test_pos_1\n",
    "    best_acc_score = test_acc\n",
    "    torch.save(LSTM_Model.state_dict(), \"best_model_lstm_victsd_priority_f1_score_pos_1.pth\")\n",
    "    print(\"save model at this epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lưu model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embed.weight',\n",
       "              tensor([[ 0.9431, -0.3020, -0.3127,  ...,  1.2127,  2.3737,  0.6159],\n",
       "                      [ 0.2120,  0.0904,  1.7937,  ..., -0.7385, -0.2394, -0.1592],\n",
       "                      [ 0.4389, -0.9409,  1.9459,  ...,  0.2650,  0.8121,  0.5084],\n",
       "                      ...,\n",
       "                      [-1.9274,  0.9808, -0.6391,  ...,  0.6170,  0.0614, -1.6429],\n",
       "                      [ 0.6131, -0.5478,  0.4856,  ..., -1.0671, -0.3419, -0.2322],\n",
       "                      [-0.4732,  0.2472,  0.1029,  ..., -0.7899, -0.0215, -0.5866]])),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[-0.1478, -0.0838, -0.1638,  ..., -0.0616,  0.0261,  0.0329],\n",
       "                      [-0.0535,  0.1666,  0.0946,  ...,  0.1306, -0.0040, -0.1347],\n",
       "                      [-0.1203,  0.0844, -0.1394,  ..., -0.1831,  0.0386,  0.0501],\n",
       "                      ...,\n",
       "                      [ 0.0938, -0.1599, -0.0761,  ..., -0.0333, -0.0064,  0.0565],\n",
       "                      [ 0.0647, -0.0707,  0.1385,  ...,  0.0527, -0.0046, -0.0162],\n",
       "                      [ 0.0425,  0.1244, -0.1459,  ...,  0.0594, -0.1174, -0.0753]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[ 0.0591,  0.0100, -0.0049,  ...,  0.0564,  0.0786,  0.1452],\n",
       "                      [-0.0767,  0.1460, -0.0845,  ...,  0.0124, -0.0122, -0.0553],\n",
       "                      [-0.1658, -0.0557, -0.0344,  ...,  0.0862,  0.1388, -0.0409],\n",
       "                      ...,\n",
       "                      [-0.1561,  0.0670,  0.1102,  ...,  0.1048,  0.0474, -0.0457],\n",
       "                      [ 0.1613,  0.0058,  0.0388,  ...,  0.1075,  0.0154,  0.0447],\n",
       "                      [ 0.0212, -0.0193, -0.0178,  ...,  0.1247,  0.0396,  0.1402]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([ 0.0095, -0.0101,  0.1169, -0.0746,  0.1081,  0.1481, -0.1271,  0.0947,\n",
       "                       0.1393, -0.0166,  0.1608,  0.0236,  0.1268, -0.1024,  0.1508,  0.1571,\n",
       "                      -0.1621, -0.0073,  0.0727, -0.1228,  0.1548,  0.1171,  0.1026,  0.1506,\n",
       "                      -0.1038, -0.0448,  0.1187,  0.1567,  0.0760, -0.0803, -0.1322,  0.0392,\n",
       "                       0.0687,  0.1563, -0.1463, -0.0483,  0.1649, -0.0297,  0.1063,  0.1519,\n",
       "                      -0.0988, -0.0160, -0.1560,  0.0180, -0.1495, -0.0225,  0.0363, -0.0486,\n",
       "                      -0.1290, -0.1294, -0.1201, -0.0547, -0.1629, -0.1107, -0.0689, -0.0461,\n",
       "                      -0.0586,  0.0431, -0.1622,  0.0336,  0.1652,  0.0149,  0.1351, -0.0972,\n",
       "                       0.1116,  0.1509,  0.1417,  0.1047, -0.0097, -0.0204,  0.0771,  0.0529,\n",
       "                      -0.1348, -0.0097,  0.0369,  0.0251, -0.1623,  0.0547,  0.0774, -0.0170,\n",
       "                      -0.0073,  0.1293,  0.1002,  0.0698, -0.1552, -0.0864,  0.1013, -0.0206,\n",
       "                      -0.0223, -0.0752, -0.0245,  0.0498, -0.1107,  0.0403, -0.0121,  0.0566,\n",
       "                       0.0328, -0.1156,  0.1449, -0.1074,  0.0820, -0.0097, -0.1043, -0.1466,\n",
       "                       0.1660,  0.1481,  0.1160, -0.0147, -0.0835,  0.0227,  0.0185,  0.1209,\n",
       "                       0.0700, -0.0245,  0.1776,  0.0038, -0.0123,  0.1324, -0.1299,  0.1437,\n",
       "                      -0.0551, -0.0968,  0.1073,  0.0661,  0.0668,  0.1592,  0.0953,  0.0328])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([-1.1797e-01, -5.0077e-02,  1.3775e-01, -1.3515e-01,  1.2590e-01,\n",
       "                      -4.7595e-02, -1.2574e-01,  7.4585e-02, -1.1057e-01, -1.9902e-02,\n",
       "                       1.3067e-01,  1.2405e-01,  1.3696e-01,  5.0195e-02,  1.6337e-01,\n",
       "                      -1.1225e-01, -5.5614e-02,  6.6003e-02,  1.0868e-01,  8.5163e-02,\n",
       "                       9.5341e-02, -3.9357e-02,  1.2393e-01,  1.7647e-01, -6.8535e-02,\n",
       "                      -1.6638e-01, -2.5671e-02,  1.2759e-01,  9.0889e-02,  5.1079e-03,\n",
       "                      -1.5429e-01, -7.2294e-02,  1.5969e-01, -1.6324e-02,  1.2911e-01,\n",
       "                      -6.6226e-02, -1.6380e-01,  1.6783e-01, -5.3062e-02, -1.5661e-01,\n",
       "                       1.8544e-03,  1.4730e-02, -9.1709e-03, -1.1450e-01, -9.0611e-03,\n",
       "                       1.7449e-01, -3.8657e-02, -1.6198e-01,  4.7329e-02,  1.6535e-01,\n",
       "                      -1.3578e-01, -1.3077e-01, -1.5229e-01,  1.5660e-01, -5.8710e-02,\n",
       "                       3.4782e-02, -1.1545e-01, -1.6456e-01, -1.2178e-01,  1.1792e-01,\n",
       "                      -1.0715e-01,  1.5006e-01, -6.0723e-02,  9.2664e-02, -4.3179e-02,\n",
       "                      -6.0915e-02,  9.0629e-03,  6.3232e-02,  4.4064e-02, -2.3720e-02,\n",
       "                       3.1649e-02, -8.5898e-05,  9.5744e-02,  1.4589e-01, -1.2163e-01,\n",
       "                       7.4376e-02,  1.2367e-01,  3.8985e-02,  9.7463e-02, -5.7591e-02,\n",
       "                      -7.7850e-02, -1.5549e-01, -7.9022e-02, -1.7709e-01,  3.6990e-02,\n",
       "                       1.7481e-01, -1.4478e-01,  3.8273e-02,  1.5686e-01, -1.6324e-01,\n",
       "                       1.0585e-03,  1.1205e-01,  3.0994e-02,  1.3358e-01,  8.2838e-02,\n",
       "                       1.0714e-01,  1.1193e-01, -8.1790e-02, -6.2237e-02,  1.0741e-01,\n",
       "                      -5.6418e-02,  1.0685e-01,  1.3235e-01, -1.7354e-01,  1.7391e-01,\n",
       "                       1.7176e-01, -3.1889e-02,  7.3814e-03, -2.1894e-02,  1.2293e-01,\n",
       "                       4.9527e-02,  1.4748e-01, -1.1693e-01,  1.7704e-01,  7.9302e-03,\n",
       "                      -1.4502e-01,  3.1513e-02,  1.1335e-01,  7.3054e-02, -1.6416e-01,\n",
       "                       3.8505e-02, -1.5400e-01,  2.0148e-02,  1.1336e-01,  1.1890e-02,\n",
       "                      -1.0999e-01, -7.1220e-02,  1.5035e-03])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[ 0.0071, -0.0014, -0.0044,  ..., -0.0100, -0.0051,  0.0119],\n",
       "                      [-0.0127,  0.0002, -0.0007,  ...,  0.0107,  0.0044, -0.0127],\n",
       "                      [ 0.0051,  0.0130, -0.0126,  ...,  0.0137,  0.0129, -0.0091],\n",
       "                      ...,\n",
       "                      [-0.0042, -0.0096,  0.0002,  ..., -0.0001, -0.0006,  0.0012],\n",
       "                      [ 0.0018, -0.0003, -0.0034,  ...,  0.0161, -0.0116,  0.0030],\n",
       "                      [ 0.0089,  0.0027,  0.0002,  ...,  0.0066,  0.0131,  0.0122]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.0027, -0.0123, -0.0150,  ...,  0.0091,  0.0130,  0.0125])),\n",
       "             ('bn1.weight',\n",
       "              tensor([1.0002, 0.9998, 1.0012,  ..., 0.9999, 1.0005, 0.9973])),\n",
       "             ('bn1.bias',\n",
       "              tensor([ 4.0250e-04,  7.2787e-05, -4.2479e-04,  ..., -5.9830e-04,\n",
       "                       4.2931e-04, -2.7899e-03])),\n",
       "             ('bn1.running_mean',\n",
       "              tensor([ 0.1577, -0.1074, -0.0213,  ...,  0.0618,  0.0232,  0.0712])),\n",
       "             ('bn1.running_var',\n",
       "              tensor([0.0034, 0.0027, 0.0026,  ..., 0.0028, 0.0027, 0.0024])),\n",
       "             ('bn1.num_batches_tracked', tensor(3270)),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 2.0020e-02, -6.4195e-03,  2.5223e-05,  ..., -1.8328e-02,\n",
       "                       -1.6939e-02,  4.1445e-03],\n",
       "                      [-1.8682e-02,  2.8893e-02,  7.0981e-03,  ...,  2.5045e-02,\n",
       "                        3.8494e-03,  4.1936e-03],\n",
       "                      [ 2.4513e-02,  2.1530e-02,  2.5306e-03,  ..., -2.6627e-03,\n",
       "                        6.3731e-03, -7.0043e-03],\n",
       "                      ...,\n",
       "                      [-2.9714e-02,  8.7784e-03, -2.9832e-02,  ..., -7.6097e-03,\n",
       "                       -2.5167e-03,  6.5600e-03],\n",
       "                      [-6.6903e-03,  1.5013e-02, -7.3807e-03,  ...,  1.4969e-02,\n",
       "                       -1.6171e-02, -2.7399e-03],\n",
       "                      [-5.1904e-03, -2.9156e-02,  2.4270e-02,  ..., -1.3052e-02,\n",
       "                        2.8159e-03, -2.1538e-02]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.0038, -0.0228, -0.0161, -0.0111, -0.0003,  0.0312,  0.0181, -0.0101,\n",
       "                      -0.0217, -0.0154,  0.0205,  0.0202, -0.0098, -0.0002, -0.0197, -0.0024,\n",
       "                      -0.0064,  0.0065, -0.0099, -0.0063,  0.0144, -0.0154,  0.0246,  0.0124,\n",
       "                      -0.0120,  0.0280, -0.0127,  0.0055, -0.0301,  0.0096,  0.0135,  0.0067,\n",
       "                      -0.0016,  0.0067,  0.0195, -0.0220,  0.0056, -0.0106, -0.0146,  0.0182,\n",
       "                      -0.0080,  0.0093, -0.0298,  0.0136,  0.0276, -0.0091,  0.0280, -0.0239,\n",
       "                      -0.0212, -0.0024, -0.0032, -0.0237, -0.0281,  0.0095, -0.0057, -0.0273,\n",
       "                      -0.0108, -0.0288, -0.0199, -0.0225,  0.0231,  0.0223, -0.0200, -0.0157,\n",
       "                       0.0266,  0.0217, -0.0169, -0.0242,  0.0169,  0.0239, -0.0194,  0.0113,\n",
       "                       0.0201, -0.0096, -0.0181, -0.0280, -0.0236, -0.0291, -0.0071, -0.0166,\n",
       "                      -0.0153,  0.0196,  0.0300, -0.0078, -0.0210,  0.0071,  0.0204, -0.0036,\n",
       "                      -0.0288, -0.0080, -0.0139, -0.0120, -0.0100,  0.0083,  0.0228, -0.0192,\n",
       "                       0.0102,  0.0151,  0.0057, -0.0306, -0.0183,  0.0060,  0.0253,  0.0118,\n",
       "                       0.0127, -0.0294,  0.0206,  0.0106, -0.0223, -0.0257, -0.0053,  0.0098,\n",
       "                       0.0152, -0.0063,  0.0229, -0.0081, -0.0285,  0.0192, -0.0111,  0.0118,\n",
       "                      -0.0005,  0.0182,  0.0125,  0.0090, -0.0088, -0.0166, -0.0217,  0.0290,\n",
       "                      -0.0008,  0.0178,  0.0309, -0.0083,  0.0308, -0.0254,  0.0063, -0.0303,\n",
       "                      -0.0214, -0.0142, -0.0215, -0.0287, -0.0028,  0.0269, -0.0243, -0.0100,\n",
       "                      -0.0067,  0.0151,  0.0246,  0.0280, -0.0112,  0.0050, -0.0303, -0.0205,\n",
       "                       0.0198, -0.0300,  0.0050,  0.0148, -0.0278, -0.0266,  0.0007, -0.0144,\n",
       "                       0.0167, -0.0011, -0.0161,  0.0128,  0.0291, -0.0038,  0.0187,  0.0177,\n",
       "                       0.0177, -0.0247,  0.0086,  0.0262, -0.0061, -0.0041, -0.0247, -0.0102,\n",
       "                      -0.0172, -0.0083,  0.0202,  0.0246, -0.0012, -0.0192, -0.0209,  0.0101,\n",
       "                       0.0180,  0.0033, -0.0056,  0.0296, -0.0203,  0.0192,  0.0058, -0.0287,\n",
       "                      -0.0236, -0.0049, -0.0303, -0.0027,  0.0072, -0.0279,  0.0082,  0.0182,\n",
       "                      -0.0274, -0.0037, -0.0108,  0.0049, -0.0059, -0.0273, -0.0073,  0.0039,\n",
       "                      -0.0056,  0.0207, -0.0212, -0.0284,  0.0228,  0.0118, -0.0189,  0.0084,\n",
       "                       0.0030, -0.0235, -0.0104, -0.0146,  0.0065,  0.0162, -0.0266,  0.0152,\n",
       "                      -0.0154,  0.0276, -0.0174,  0.0285, -0.0215,  0.0067, -0.0298,  0.0267,\n",
       "                       0.0247, -0.0257,  0.0083, -0.0135,  0.0041, -0.0020,  0.0298,  0.0211,\n",
       "                       0.0249,  0.0102, -0.0146, -0.0302,  0.0073, -0.0150, -0.0233, -0.0140,\n",
       "                       0.0137,  0.0264,  0.0249, -0.0072, -0.0282, -0.0009,  0.0189, -0.0091])),\n",
       "             ('bn2.weight',\n",
       "              tensor([0.9998, 0.9997, 0.9976, 0.9978, 0.9986, 0.9969, 0.9996, 1.0008, 1.0008,\n",
       "                      0.9982, 0.9995, 1.0006, 0.9990, 0.9987, 0.9984, 1.0022, 1.0000, 1.0006,\n",
       "                      0.9989, 0.9974, 1.0020, 1.0002, 1.0006, 1.0011, 1.0010, 0.9997, 0.9978,\n",
       "                      0.9979, 0.9998, 0.9980, 0.9999, 0.9982, 0.9993, 0.9974, 0.9983, 0.9973,\n",
       "                      0.9995, 1.0045, 0.9978, 1.0005, 0.9992, 1.0015, 1.0007, 1.0013, 1.0020,\n",
       "                      1.0003, 1.0000, 0.9997, 1.0006, 0.9988, 1.0007, 1.0004, 0.9998, 0.9980,\n",
       "                      0.9981, 1.0013, 0.9998, 1.0043, 0.9995, 0.9998, 1.0003, 0.9976, 0.9991,\n",
       "                      0.9971, 1.0014, 1.0005, 0.9996, 0.9979, 1.0029, 1.0008, 1.0027, 0.9983,\n",
       "                      0.9965, 1.0020, 0.9994, 1.0017, 0.9994, 0.9983, 1.0015, 1.0018, 1.0018,\n",
       "                      0.9994, 0.9991, 1.0045, 0.9981, 0.9985, 1.0012, 1.0001, 1.0002, 0.9974,\n",
       "                      0.9993, 1.0005, 1.0004, 1.0037, 0.9955, 1.0058, 0.9998, 1.0019, 0.9988,\n",
       "                      0.9979, 1.0011, 0.9977, 0.9964, 0.9985, 0.9986, 0.9988, 0.9962, 0.9997,\n",
       "                      1.0033, 1.0002, 0.9992, 0.9988, 1.0001, 0.9982, 0.9976, 1.0030, 0.9993,\n",
       "                      0.9973, 0.9995, 1.0020, 1.0009, 0.9980, 0.9972, 0.9963, 1.0042, 0.9999,\n",
       "                      0.9969, 1.0004, 1.0045, 0.9994, 1.0019, 1.0017, 0.9986, 0.9992, 0.9981,\n",
       "                      0.9984, 0.9975, 0.9985, 0.9996, 0.9979, 0.9975, 0.9992, 1.0006, 0.9983,\n",
       "                      0.9995, 1.0027, 0.9989, 0.9997, 0.9990, 1.0005, 1.0019, 0.9981, 0.9983,\n",
       "                      1.0021, 0.9996, 1.0080, 0.9973, 0.9981, 1.0008, 0.9990, 0.9983, 1.0053,\n",
       "                      1.0008, 0.9998, 0.9984, 0.9993, 0.9983, 0.9999, 1.0002, 0.9984, 1.0012,\n",
       "                      0.9994, 0.9992, 0.9962, 0.9991, 0.9987, 0.9994, 1.0013, 1.0001, 0.9977,\n",
       "                      1.0001, 1.0005, 1.0003, 0.9981, 0.9977, 0.9987, 0.9970, 1.0055, 0.9989,\n",
       "                      0.9967, 0.9992, 0.9999, 1.0015, 0.9993, 0.9968, 0.9962, 0.9980, 1.0002,\n",
       "                      1.0001, 1.0004, 1.0009, 0.9987, 1.0038, 0.9987, 0.9990, 1.0006, 0.9993,\n",
       "                      1.0025, 1.0039, 0.9983, 0.9963, 1.0007, 1.0008, 1.0012, 0.9998, 1.0068,\n",
       "                      0.9968, 0.9990, 0.9966, 0.9987, 0.9998, 0.9983, 1.0004, 0.9996, 0.9979,\n",
       "                      0.9974, 0.9980, 1.0016, 0.9975, 1.0012, 0.9982, 0.9987, 1.0035, 0.9991,\n",
       "                      0.9980, 1.0001, 1.0014, 1.0046, 0.9999, 1.0024, 1.0006, 0.9990, 0.9990,\n",
       "                      0.9971, 0.9992, 1.0037, 0.9986, 0.9993, 0.9990, 1.0012, 0.9996, 0.9976,\n",
       "                      0.9984, 0.9972, 1.0025, 0.9975])),\n",
       "             ('bn2.bias',\n",
       "              tensor([-7.5010e-05, -3.1722e-04, -2.7412e-03, -2.4722e-03,  2.2990e-04,\n",
       "                      -3.7132e-03, -5.7416e-04,  1.2213e-03,  1.0288e-03, -3.6008e-03,\n",
       "                       7.7680e-04, -7.9470e-04, -5.8878e-05, -1.5158e-03, -1.0266e-03,\n",
       "                       1.9459e-03,  5.9764e-04,  6.2953e-04, -7.0710e-04, -1.4192e-03,\n",
       "                       1.3081e-03,  2.1477e-03,  8.6379e-04,  1.0089e-03,  8.5989e-04,\n",
       "                       1.8530e-03, -1.8274e-03, -1.2738e-03, -1.8847e-04, -1.1181e-03,\n",
       "                      -3.1234e-04, -9.8880e-04, -1.8688e-04, -4.1577e-03, -1.3835e-03,\n",
       "                      -3.1038e-03, -2.3120e-03,  3.8839e-03, -1.3607e-03, -1.4861e-03,\n",
       "                      -4.1770e-04,  1.9525e-03,  2.9166e-04,  1.3897e-04,  1.3658e-03,\n",
       "                       5.7517e-04,  5.7118e-04, -1.8321e-04,  1.6722e-03, -2.1231e-03,\n",
       "                       3.9617e-04,  8.9549e-04,  7.9478e-04, -1.3697e-03, -2.1799e-03,\n",
       "                      -7.2580e-04,  8.1246e-04,  6.9684e-04, -1.4073e-03, -1.7116e-05,\n",
       "                       8.3534e-04, -2.2441e-03, -1.2312e-03, -2.1753e-03,  1.9940e-03,\n",
       "                      -8.6223e-05,  6.9332e-04, -1.8676e-03,  1.3602e-03,  3.5704e-04,\n",
       "                       1.2900e-03, -8.4847e-04, -2.2297e-03,  2.3916e-03, -7.0278e-04,\n",
       "                       2.2027e-03, -1.3318e-03, -1.2051e-03, -1.1578e-04,  7.5386e-04,\n",
       "                       3.9970e-04, -2.2775e-04,  4.0183e-04,  1.4170e-03, -1.1315e-03,\n",
       "                      -2.7785e-03,  1.0179e-03,  3.8748e-04,  1.0509e-03, -2.0725e-03,\n",
       "                      -3.3798e-03,  8.8405e-04, -9.0238e-05,  8.2580e-04, -2.5179e-03,\n",
       "                       3.1205e-03,  9.5781e-04, -1.1917e-03,  8.8901e-05, -1.3433e-03,\n",
       "                      -3.9751e-04, -1.2437e-03, -2.7919e-03, -1.2964e-03, -7.9384e-04,\n",
       "                      -1.6535e-04, -3.8115e-03, -3.8548e-04,  3.1473e-03,  2.7402e-03,\n",
       "                      -9.4523e-04, -3.5695e-04, -7.6196e-04, -1.8035e-03, -1.0248e-03,\n",
       "                      -1.6890e-04, -1.4632e-04, -2.0574e-03, -2.0491e-04,  2.2826e-03,\n",
       "                       7.1313e-05, -4.8532e-04, -1.6712e-03, -3.1010e-03,  2.3207e-03,\n",
       "                       6.9176e-04, -2.5455e-03, -1.0923e-03, -8.4699e-04,  5.1425e-04,\n",
       "                       1.5404e-03,  1.1527e-04, -7.0335e-04, -2.8511e-04, -1.1258e-03,\n",
       "                      -1.5725e-03, -1.6633e-03, -1.4630e-03, -2.4381e-04, -1.1595e-03,\n",
       "                      -2.7439e-03,  2.1597e-04,  1.3626e-03, -1.8739e-03,  7.0228e-04,\n",
       "                       3.2340e-03, -1.6357e-03, -6.8434e-04, -9.2989e-04,  1.3483e-03,\n",
       "                       2.1544e-03, -1.6328e-03, -1.9037e-03,  1.7149e-03, -1.6333e-04,\n",
       "                       3.7300e-03,  5.6544e-04, -1.4294e-04,  3.9485e-04, -1.0643e-03,\n",
       "                       9.7402e-05,  2.0620e-03, -4.8667e-04, -1.0425e-03, -1.4946e-03,\n",
       "                      -1.1473e-03, -1.3275e-03, -9.5487e-04,  1.1405e-04, -1.3999e-03,\n",
       "                       6.8706e-04, -7.8045e-04, -1.6258e-03, -1.5408e-03, -5.1100e-04,\n",
       "                       1.8854e-04, -1.2058e-03,  2.2963e-03,  6.8224e-04, -4.1407e-03,\n",
       "                      -4.9029e-04,  1.0710e-03,  1.2905e-03, -1.3800e-03, -2.4075e-03,\n",
       "                      -4.4036e-04, -3.1581e-03,  2.0338e-03,  1.3568e-03, -1.7534e-03,\n",
       "                       2.4388e-04,  6.9194e-04,  2.1069e-03, -3.9607e-04, -2.3280e-03,\n",
       "                      -1.6399e-03, -9.4901e-04,  1.3086e-03,  1.2132e-03,  2.1544e-03,\n",
       "                       1.8408e-03, -1.0683e-03,  1.7995e-03,  2.6337e-05,  4.9555e-04,\n",
       "                      -3.9692e-05, -8.2965e-04,  1.8040e-03,  9.6808e-04, -1.9018e-03,\n",
       "                      -1.3833e-03, -8.7492e-05, -2.3076e-03,  1.8891e-03, -4.4930e-04,\n",
       "                       2.0883e-03, -2.4864e-03,  7.8170e-04, -2.7435e-03, -8.6082e-04,\n",
       "                      -1.1857e-03, -8.4408e-04,  5.3628e-04, -1.3179e-03,  8.2812e-04,\n",
       "                       1.3122e-03, -1.6362e-03,  1.0516e-04, -1.5589e-03,  2.0127e-03,\n",
       "                      -1.6235e-03,  1.6247e-03,  2.9610e-03, -1.5354e-04, -1.1357e-03,\n",
       "                      -1.6178e-04, -1.9115e-04,  8.2502e-04, -1.4843e-03,  2.3292e-03,\n",
       "                       1.8204e-03,  1.1591e-03, -1.2786e-03, -1.9728e-03, -1.2546e-03,\n",
       "                       3.1017e-04, -8.5990e-04,  6.2690e-05, -1.0557e-03, -4.6214e-04,\n",
       "                      -1.7955e-04, -1.2833e-03,  6.4478e-05, -3.3338e-03,  1.1915e-03,\n",
       "                      -2.8330e-03])),\n",
       "             ('bn2.running_mean',\n",
       "              tensor([-0.1611,  0.1845,  0.0119, -0.2734,  0.0194, -0.0197,  0.0203,  0.1542,\n",
       "                       0.1839, -0.2563,  0.1837, -0.0512, -0.2437, -0.0064,  0.2073,  0.4539,\n",
       "                      -0.1575, -0.1066,  0.2766, -0.0964,  0.5397,  0.0181,  0.0806,  0.2635,\n",
       "                      -0.0127, -0.5391, -0.2052, -0.0808,  0.1257, -0.3899, -0.2707, -0.4876,\n",
       "                      -0.0127, -0.1076, -0.0699, -0.2631,  0.2413,  0.2616,  0.0186, -0.2363,\n",
       "                       0.1529, -0.2217, -0.1004, -0.2298,  0.1094, -0.1558,  0.2014, -0.3455,\n",
       "                       0.0029,  0.0051,  0.1720, -0.2974,  0.1235, -0.0958, -0.2149,  0.0064,\n",
       "                      -0.2656,  0.1868, -0.3754,  0.4105,  0.0057,  0.2734, -0.6168,  0.0029,\n",
       "                      -0.2409, -0.0060,  0.3160, -0.1754,  0.0009,  0.2863,  0.2551,  0.3503,\n",
       "                       0.1629, -0.0624,  0.0252, -0.2293, -0.1876, -0.2329,  0.1775,  0.0484,\n",
       "                      -0.3059,  0.0564, -0.0048, -0.2177, -0.4663, -0.0037,  0.1855,  0.2233,\n",
       "                      -0.0432,  0.2977,  0.1394,  0.0047,  0.1093,  0.5859, -0.1423,  0.0771,\n",
       "                      -0.0133, -0.0051,  0.1083,  0.1300,  0.0258,  0.0855,  0.1829, -0.1674,\n",
       "                       0.2893,  0.3991, -0.0438,  0.2049, -0.2374, -0.2950, -0.1947, -0.1023,\n",
       "                       0.2794,  0.5365, -0.2369, -0.0817, -0.3723, -0.0231, -0.0378, -0.1231,\n",
       "                       0.0482, -0.1575,  0.1124, -0.4305,  0.7012, -0.3059,  0.3098, -0.0620,\n",
       "                      -0.0618,  0.2703,  0.1787, -0.1776, -0.3797,  0.2338, -0.3854,  0.4660,\n",
       "                      -0.3534, -0.0479, -0.0624,  0.5361,  0.2202,  0.2954, -0.1127,  0.1646,\n",
       "                      -0.2584, -0.1648, -0.2725,  0.2075, -0.1627,  0.2892,  0.2302, -0.4450,\n",
       "                      -0.0639,  0.1478,  0.0434,  0.1325,  0.0562, -0.0816,  0.0909,  0.0159,\n",
       "                       0.2759, -0.1174,  0.0972, -0.1180,  0.0112,  0.1641, -0.3861, -0.4854,\n",
       "                       0.0189,  0.2289,  0.2048, -0.2741,  0.0282,  0.1981,  0.2736, -0.2239,\n",
       "                      -0.1917, -0.0019,  0.0656,  0.2610, -0.1004, -0.0058,  0.3036, -0.0577,\n",
       "                       0.4810, -0.4031,  0.5103,  0.1175, -0.2133, -0.0261, -0.5689, -0.7686,\n",
       "                      -0.4568, -0.2135, -0.1339,  0.1727,  0.0981,  0.0226, -0.4261, -0.0017,\n",
       "                       0.2509, -0.2339, -0.3137, -0.7227, -0.0397,  0.3454,  0.2115, -0.0848,\n",
       "                       0.0429, -0.1157,  0.2074, -0.1796, -0.0825, -0.1723, -0.1013, -0.0948,\n",
       "                      -0.0038, -0.3151, -0.0521, -0.0563, -0.1626, -0.5077, -0.2019, -0.2507,\n",
       "                      -0.2136, -0.0748, -0.2315, -0.1950, -0.2334,  0.1173,  0.3732,  0.0988,\n",
       "                       0.0414,  0.0588,  0.3156,  0.0526,  0.2852,  0.1731,  0.0251,  0.2206,\n",
       "                       0.0134,  0.2513, -0.0947, -0.3814, -0.1088, -0.0075,  0.2899,  0.0433,\n",
       "                      -0.0516,  0.0284,  0.1657, -0.1185,  0.2347,  0.0370, -0.3232, -0.2248])),\n",
       "             ('bn2.running_var',\n",
       "              tensor([0.1721, 0.2221, 0.1765, 0.1812, 0.1510, 0.1926, 0.2027, 0.1581, 0.1592,\n",
       "                      0.1685, 0.2089, 0.1665, 0.1666, 0.1559, 0.1531, 0.2040, 0.1484, 0.1606,\n",
       "                      0.1791, 0.1538, 0.2081, 0.1750, 0.1603, 0.1782, 0.1723, 0.1984, 0.1740,\n",
       "                      0.1429, 0.1797, 0.1608, 0.1681, 0.1733, 0.1637, 0.1485, 0.1979, 0.1891,\n",
       "                      0.2121, 0.2090, 0.1749, 0.1955, 0.1685, 0.1819, 0.1552, 0.1794, 0.1567,\n",
       "                      0.2251, 0.1716, 0.1763, 0.1633, 0.1635, 0.2157, 0.1579, 0.1818, 0.1845,\n",
       "                      0.1987, 0.2172, 0.1897, 0.1955, 0.1781, 0.1935, 0.1785, 0.1804, 0.2398,\n",
       "                      0.1489, 0.2095, 0.1660, 0.1916, 0.1833, 0.1841, 0.1939, 0.2166, 0.1577,\n",
       "                      0.1737, 0.2250, 0.2459, 0.1559, 0.1647, 0.1513, 0.2007, 0.1883, 0.1914,\n",
       "                      0.1784, 0.1618, 0.1744, 0.1815, 0.1629, 0.2087, 0.1591, 0.2261, 0.1763,\n",
       "                      0.1865, 0.1692, 0.1644, 0.1812, 0.1521, 0.1875, 0.1565, 0.1678, 0.1836,\n",
       "                      0.1495, 0.1920, 0.1730, 0.1561, 0.1444, 0.1630, 0.1834, 0.1485, 0.2072,\n",
       "                      0.1619, 0.2269, 0.1907, 0.2059, 0.1900, 0.2280, 0.1632, 0.1583, 0.1795,\n",
       "                      0.1895, 0.1663, 0.2796, 0.1928, 0.1762, 0.1841, 0.1843, 0.2449, 0.1643,\n",
       "                      0.1926, 0.1500, 0.1800, 0.1714, 0.1836, 0.1885, 0.1639, 0.1536, 0.2170,\n",
       "                      0.2672, 0.2477, 0.1884, 0.1591, 0.2034, 0.1679, 0.2014, 0.1859, 0.1853,\n",
       "                      0.1746, 0.1983, 0.1678, 0.1690, 0.1719, 0.1867, 0.1615, 0.1841, 0.1484,\n",
       "                      0.1766, 0.1701, 0.1869, 0.1678, 0.1850, 0.1582, 0.1728, 0.2299, 0.1919,\n",
       "                      0.1901, 0.1779, 0.1621, 0.1798, 0.2087, 0.2184, 0.1558, 0.1645, 0.1748,\n",
       "                      0.1946, 0.1682, 0.1600, 0.2462, 0.1507, 0.1667, 0.1594, 0.1564, 0.2082,\n",
       "                      0.1546, 0.1771, 0.2077, 0.2186, 0.1728, 0.1777, 0.1982, 0.1692, 0.1897,\n",
       "                      0.1979, 0.2188, 0.1848, 0.1538, 0.1911, 0.1710, 0.1508, 0.1493, 0.2071,\n",
       "                      0.1863, 0.1643, 0.1579, 0.1533, 0.1735, 0.2401, 0.1724, 0.2300, 0.1803,\n",
       "                      0.2106, 0.1666, 0.2067, 0.1623, 0.1483, 0.1784, 0.1995, 0.1532, 0.1585,\n",
       "                      0.1982, 0.1744, 0.1756, 0.1812, 0.1605, 0.1946, 0.1603, 0.2194, 0.1941,\n",
       "                      0.1650, 0.1832, 0.1755, 0.1837, 0.1634, 0.1500, 0.1654, 0.2056, 0.1391,\n",
       "                      0.1790, 0.1641, 0.1634, 0.1821, 0.1636, 0.1493, 0.2147, 0.1489, 0.1506,\n",
       "                      0.2342, 0.1608, 0.1939, 0.1754, 0.1781, 0.2243, 0.1719, 0.1545, 0.1652,\n",
       "                      0.1843, 0.2019, 0.1651, 0.1858])),\n",
       "             ('bn2.num_batches_tracked', tensor(3270)),\n",
       "             ('fc3.weight',\n",
       "              tensor([[-0.0428,  0.0313, -0.0458,  ...,  0.0083,  0.0609, -0.0509],\n",
       "                      [ 0.0442, -0.0298, -0.0184,  ..., -0.0144, -0.0430,  0.0339],\n",
       "                      [-0.0564,  0.0234,  0.0330,  ..., -0.0167,  0.0432, -0.0527],\n",
       "                      ...,\n",
       "                      [-0.0221, -0.0096,  0.0074,  ...,  0.0081, -0.0334, -0.0029],\n",
       "                      [ 0.0167, -0.0325, -0.0149,  ..., -0.0482, -0.0037, -0.0492],\n",
       "                      [-0.0384,  0.0231,  0.0403,  ...,  0.0311,  0.0546, -0.0409]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([-0.0518, -0.0070,  0.0371,  0.0354,  0.0542, -0.0160,  0.0302, -0.0447,\n",
       "                       0.0562,  0.0567,  0.0436, -0.0552,  0.0226, -0.0061, -0.0548,  0.0031,\n",
       "                       0.0553, -0.0289, -0.0407, -0.0110, -0.0202, -0.0033,  0.0541, -0.0393,\n",
       "                      -0.0256,  0.0359, -0.0128, -0.0284, -0.0017,  0.0278, -0.0471,  0.0050,\n",
       "                      -0.0411, -0.0056,  0.0013, -0.0230, -0.0387, -0.0383,  0.0369,  0.0603,\n",
       "                      -0.0551, -0.0065, -0.0521,  0.0151, -0.0349,  0.0272,  0.0393, -0.0516,\n",
       "                      -0.0161, -0.0559,  0.0391, -0.0563,  0.0056, -0.0032, -0.0341, -0.0147,\n",
       "                      -0.0442, -0.0532, -0.0577,  0.0119,  0.0241,  0.0063,  0.0386, -0.0221])),\n",
       "             ('bn3.weight',\n",
       "              tensor([0.9730, 1.0350, 0.9743, 1.0397, 1.0367, 1.0402, 1.0398, 0.9801, 0.9730,\n",
       "                      1.0374, 0.9805, 0.9726, 1.0382, 0.9738, 0.9783, 0.9759, 0.9732, 1.0353,\n",
       "                      1.0435, 1.0448, 1.0345, 0.9782, 0.9759, 0.9785, 1.0364, 0.9777, 1.0404,\n",
       "                      1.0371, 0.9770, 1.0366, 1.0373, 1.0372, 1.0393, 1.0376, 0.9776, 1.0348,\n",
       "                      0.9756, 1.0369, 0.9759, 0.9711, 0.9798, 1.0341, 0.9721, 0.9809, 1.0383,\n",
       "                      1.0395, 1.0406, 0.9834, 1.0416, 1.0413, 1.0369, 1.0408, 0.9767, 1.0404,\n",
       "                      0.9769, 1.0376, 1.0007, 0.9797, 1.0363, 0.9741, 1.0422, 1.0410, 1.0425,\n",
       "                      0.9722])),\n",
       "             ('bn3.bias',\n",
       "              tensor([-0.0332,  0.0429, -0.0350,  0.0432,  0.0429,  0.0460,  0.0483, -0.0320,\n",
       "                      -0.0320,  0.0446, -0.0309, -0.0344,  0.0453, -0.0323, -0.0317, -0.0326,\n",
       "                      -0.0339,  0.0437,  0.0518,  0.0533,  0.0428, -0.0322, -0.0308, -0.0290,\n",
       "                       0.0437, -0.0261,  0.0437,  0.0415, -0.0311,  0.0438,  0.0414,  0.0419,\n",
       "                       0.0448,  0.0448, -0.0288,  0.0429, -0.0308,  0.0440, -0.0308, -0.0358,\n",
       "                      -0.0304,  0.0417, -0.0324, -0.0233,  0.0459,  0.0470,  0.0453, -0.0180,\n",
       "                       0.0488,  0.0477,  0.0440,  0.0496, -0.0312,  0.0496, -0.0305,  0.0428,\n",
       "                       0.0012, -0.0296,  0.0407, -0.0296,  0.0481,  0.0450,  0.0527, -0.0362])),\n",
       "             ('bn3.running_mean',\n",
       "              tensor([-0.1701,  0.0954,  0.1404, -0.1903,  0.3827,  0.1678, -0.2940, -0.4242,\n",
       "                       0.2967, -0.0815,  0.2650, -0.4151,  0.1231,  0.0793, -0.0455, -0.2261,\n",
       "                      -0.1444,  0.1473, -0.1267,  0.0174, -0.2623,  0.0312, -0.0453, -0.1769,\n",
       "                      -0.2509,  0.3203, -0.2840, -0.0928,  0.2885, -0.2901, -0.1515,  0.3709,\n",
       "                      -0.1292, -0.1357, -0.0247,  0.0978,  0.3021, -0.4433,  0.3151, -0.0953,\n",
       "                      -0.0896, -0.1620, -0.2339, -0.0901, -0.0738,  0.3004, -0.0557,  0.0174,\n",
       "                      -0.2460,  0.1967,  0.2625, -0.1726,  0.2684,  0.3158,  0.1693,  0.0306,\n",
       "                       0.0221,  0.2578, -0.1865,  0.1095, -0.1613,  0.0113, -0.1886, -0.7250])),\n",
       "             ('bn3.running_var',\n",
       "              tensor([0.1881, 0.1390, 0.2028, 0.2646, 0.1436, 0.2107, 0.1528, 0.2530, 0.1436,\n",
       "                      0.1735, 0.2707, 0.1920, 0.1913, 0.1690, 0.2232, 0.2064, 0.1917, 0.1545,\n",
       "                      0.1728, 0.1842, 0.1879, 0.1942, 0.1605, 0.1711, 0.1991, 0.1412, 0.2595,\n",
       "                      0.2288, 0.2142, 0.1480, 0.2356, 0.1929, 0.2091, 0.1784, 0.1860, 0.2182,\n",
       "                      0.1665, 0.1648, 0.2188, 0.2496, 0.2111, 0.1642, 0.1791, 0.1690, 0.1816,\n",
       "                      0.1677, 0.1643, 0.2089, 0.1488, 0.1696, 0.1758, 0.1641, 0.1795, 0.1398,\n",
       "                      0.1605, 0.1818, 0.1593, 0.2180, 0.2040, 0.1777, 0.1680, 0.1871, 0.1609,\n",
       "                      0.2596])),\n",
       "             ('bn3.num_batches_tracked', tensor(3270)),\n",
       "             ('fc4.weight',\n",
       "              tensor([[ 0.0410, -0.0796,  0.0739, -0.1159, -0.0773, -0.0615, -0.0477,  0.1005,\n",
       "                        0.0355, -0.0846,  0.0896,  0.0575, -0.0839,  0.0323,  0.0834,  0.0806,\n",
       "                        0.0719, -0.0505, -0.0474, -0.0396, -0.1430,  0.0844,  0.0358,  0.0267,\n",
       "                       -0.0827,  0.0054, -0.1462, -0.1437,  0.0932, -0.0823, -0.1429, -0.1049,\n",
       "                       -0.0873, -0.0807,  0.0163, -0.0898,  0.0375, -0.0763,  0.0388,  0.0653,\n",
       "                        0.0442, -0.1160,  0.0252,  0.0038, -0.0755, -0.0526, -0.0676, -0.0059,\n",
       "                       -0.0309, -0.0617, -0.0688, -0.0435,  0.0552, -0.0298,  0.0360, -0.1085,\n",
       "                       -0.0143,  0.0323, -0.1247,  0.0160, -0.0531, -0.0847, -0.0312,  0.0722]])),\n",
       "             ('fc4.bias', tensor([-0.0059]))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_Model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=50000\n",
    "embedding_dim=128\n",
    "max_length=128\n",
    "predict_model = LSTMModel(vocal_size=vocab_size, embedding_dim=embedding_dim,hidden_size=32, length=max_length, output_dim=1)\n",
    "predict_model.load_state_dict(torch.load('best_model_lstm_victsd_priority_f1_score_pos_1.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dự đoán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocess test with 3 type: ViCTSD, ViHSD, ViMergre\n",
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE=32\n",
    "\n",
    "def test_dataloader(dataset_test_type):\n",
    "    test = load_data(set_name='test', dataset=dataset_test_type)\n",
    "    test['label'] = test['label'].replace(2,1)\n",
    "    test['text'] = test['text'].apply(tokenize)\n",
    "    test_preprocess = preprocess_data(test,\n",
    "                                    url=True,\n",
    "                                    punctuation=True,\n",
    "                                    lowercase=True,\n",
    "                                    stopword=True,\n",
    "                                    special_stopwords=special_stopwords,\n",
    "                                    emoji=True)\n",
    "    X_test = test_preprocess['text'].astype(str)\n",
    "    y_test = test_preprocess['label']\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test = pad_sequences(X_test, maxlen=max_length, padding='post',truncating='post')\n",
    "    test_data = CustomDataset(X_test, y_test)\n",
    "    test_dataloader = DataLoader(dataset=test_data,\n",
    "                                    batch_sampler=StratifiedBatchSampler(torch.tensor(test_data.y_encoded), batch_size=BATCH_SIZE))\n",
    "    return test_dataloader\n",
    "\n",
    "vihsd_test = test_dataloader('vihsd')\n",
    "victsd_test = test_dataloader('victsd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0201,   Test acc: 0.7820,   F1 test score: 0.1571, F1 test macro: 0.5212\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = 0,0\n",
    "f1_test_pos_1 = []\n",
    "f1_macro = []\n",
    "predict_model.eval()\n",
    "y_pred_list = []\n",
    "y_real_list = []\n",
    "loss_fn = nn.BCELoss()\n",
    "for batch, (X_test_, y_test_) in enumerate(vihsd_test):\n",
    "    # 1. Forward pass\n",
    "    X_test_ = X_test_.long()\n",
    "    test_pred = predict_model(X_test_)\n",
    "\n",
    "    y_pred_list.append(test_pred.tolist())\n",
    "    y_real_list.append(y_test_.tolist())\n",
    "\n",
    "    # 2. Calculate the loss (accumulatively)\n",
    "    test_loss += loss_fn(test_pred.squeeze(dim=1), y_test_.float()).item()\n",
    "    # print(test_pred.shape)\n",
    "    # 3. Calculate accuracy\n",
    "    acc = accuracy_fn(y_true= y_test_.float(),\n",
    "                    y_pred = test_pred.squeeze(dim=1))\n",
    "    acc = (torch.round(test_pred)==y_test_).sum().item()/len(y_test_)\n",
    "    test_acc += acc\n",
    "    # 4. Calculate f1 score\n",
    "    pos_1, macro = f1_score_fn(y_true= y_test_.float(),\n",
    "                            y_pred = test_pred.squeeze(dim=1))\n",
    "    f1_test_pos_1.append(pos_1.item())\n",
    "    f1_macro.append(macro.item())\n",
    "# Calculate the test loss average per batch\n",
    "test_loss /= len(vihsd_test)\n",
    "# test_loss /= len(test_dataloader)\n",
    "f1_test_pos_1 = sum(f1_test_pos_1)/len(f1_test_pos_1)\n",
    "f1_test_macro = sum(f1_macro)/len(f1_macro)\n",
    "# Calculate the test acc average per batch\n",
    "test_acc /= len(vihsd_test)\n",
    "print(f\"Test loss: {test_loss:.4f},   Test acc: {test_acc:.4f},   F1 test score: {f1_test_pos_1:.4f}, F1 test macro: {f1_test_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViHSD: Test loss: 0.0201,   Test acc: 0.7819,   F1 test score: 0.1623, F1 test macro: 0.5238\n",
    "victsd_test: Test loss: 0.0197,   Test acc: 0.7891,   F1 test score: 0.1205, F1 test macro: 0.5006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_y_pred_list = sum(y_pred_list, [])\n",
    "flat_y_pred_list = sum(flat_y_pred_list, [])\n",
    "flat_y_pred_list = [round(x) for x in flat_y_pred_list]\n",
    "flat_y_real_list = sum(y_real_list,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.89      5548\n",
      "           1       0.29      0.12      0.17      1132\n",
      "\n",
      "    accuracy                           0.80      6680\n",
      "   macro avg       0.56      0.53      0.53      6680\n",
      "weighted avg       0.75      0.80      0.77      6680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true=flat_y_real_list, y_pred=flat_y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
